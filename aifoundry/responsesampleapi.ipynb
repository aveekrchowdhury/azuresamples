{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d341ffb7",
   "metadata": {},
   "source": [
    "# Parallel vs Sequential Tool Calls with Azure OpenAI Response API\n",
    "\n",
    "This notebook demonstrates two different execution patterns:\n",
    "1. **Parallel Tool Calls**: Multiple tools execute simultaneously (default behavior)\n",
    "2. **Sequential Tool Calls**: Tools execute one after another in a specific order\n",
    "\n",
    "Both patterns have their use cases depending on whether tool results depend on each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0946a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: ‚úÖ Yes\n",
      "API Key (first 10 chars): DXDrtFZ92E...\n",
      "Base URL: https://aifoundryarc.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if the API key is loaded\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "print(f\"API Key loaded: {'‚úÖ Yes' if api_key else '‚ùå No'}\")\n",
    "print(f\"API Key (first 10 chars): {api_key[:10]}...\" if api_key else \"API Key: Not found\")\n",
    "\n",
    "# Show the base URL that will be used\n",
    "base_url = \"https://aifoundryarc.openai.azure.com/\"\n",
    "print(f\"Base URL: {base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e29c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client created successfully\n",
      "üîÑ Streaming response:\n",
      "üîÑ Streaming response:\n",
      "Got it ‚Äî test received. How can I help or what would you like to test next?Got it ‚Äî test received. How can I help or what would you like to test next?"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create the OpenAI client with Azure configuration\n",
    "try:\n",
    "    client = OpenAI(  \n",
    "      base_url = \"https://aifoundryarc.openai.azure.com/openai/v1\",\n",
    "      api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "    )\n",
    "    print(\"‚úÖ OpenAI client created successfully\")\n",
    "    \n",
    "    # Make the response API call\n",
    "    response = client.responses.create(\n",
    "        input = \"This is a test\",\n",
    "        model = \"gpt-5-mini\", # replace with model deployment name\n",
    "        stream = True\n",
    "    )\n",
    "    \n",
    "    print(\"üîÑ Streaming response:\")\n",
    "    for event in response:\n",
    "        if event.type == 'response.output_text.delta':\n",
    "            print(event.delta, end='')\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Provide troubleshooting suggestions\n",
    "    if \"api_key\" in str(e).lower():\n",
    "        print(\"\\nüí° Troubleshooting: API key issue\")\n",
    "        print(\"   - Make sure the .env file has AZURE_OPENAI_API_KEY set\")\n",
    "        print(\"   - Run the environment setup cell first\")\n",
    "    elif \"model\" in str(e).lower():\n",
    "        print(\"\\nüí° Troubleshooting: Model issue\")\n",
    "        print(\"   - Check if 'gpt-5-mini' is the correct model name\")\n",
    "        print(\"   - Verify the model is deployed in your Azure OpenAI resource\")\n",
    "    elif \"endpoint\" in str(e).lower() or \"base_url\" in str(e).lower():\n",
    "        print(\"\\nüí° Troubleshooting: Endpoint issue\")\n",
    "        print(\"   - Verify the base_url matches your Azure OpenAI endpoint\")\n",
    "        print(\"   - Check if the endpoint supports the responses API\")\n",
    "    else:\n",
    "        print(\"\\nüí° General troubleshooting:\")\n",
    "        print(\"   - Check your Azure OpenAI resource configuration\")\n",
    "        print(\"   - Verify the API version and model deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9436e314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"resp_68d208e609f48193a34b1bcea1f4e225087032dd1462be23\",\n",
      "  \"created_at\": 1758595302.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4o\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"location\\\":\\\"San Francisco\\\"}\",\n",
      "      \"call_id\": \"call_rX4bQjg9YKUZbmNbVmVJxk6p\",\n",
      "      \"name\": \"get_weather\",\n",
      "      \"type\": \"function_call\",\n",
      "      \"id\": \"fc_68d208e69b7481938d2049a726acb547087032dd1462be23\",\n",
      "      \"status\": \"completed\"\n",
      "    },\n",
      "    {\n",
      "      \"arguments\": \"{\\\"location\\\":\\\"New York\\\"}\",\n",
      "      \"call_id\": \"call_UwlSjsF0UsKAxYZOa0Fkyik9\",\n",
      "      \"name\": \"get_weather\",\n",
      "      \"type\": \"function_call\",\n",
      "      \"id\": \"fc_68d208e6bc6c8193a1ab6c0cebbe9521087032dd1462be23\",\n",
      "      \"status\": \"completed\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"name\": \"get_weather\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"location\": {\n",
      "            \"type\": \"string\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"location\"\n",
      "        ],\n",
      "        \"additionalProperties\": false\n",
      "      },\n",
      "      \"strict\": true,\n",
      "      \"type\": \"function\",\n",
      "      \"description\": \"Get the weather for a location\"\n",
      "    }\n",
      "  ],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"conversation\": null,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": null,\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": null\n",
      "  },\n",
      "  \"top_logprobs\": null,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 48,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 46,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 94\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"content_filters\": null,\n",
      "  \"store\": true\n",
      "}\n",
      "{\n",
      "  \"id\": \"resp_68d208e6e20c819399e873ba35227e1c087032dd1462be23\",\n",
      "  \"created_at\": 1758595302.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4o\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"msg_68d208e7290081938de981f80aa356e4087032dd1462be23\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"In San Francisco, the temperature is currently 70 degrees. Similarly, New York also has a temperature of 70 degrees.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"conversation\": null,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": \"resp_68d208e609f48193a34b1bcea1f4e225087032dd1462be23\",\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": null,\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": null\n",
      "  },\n",
      "  \"top_logprobs\": null,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 73,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 26,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 99\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"content_filters\": null,\n",
      "  \"store\": true\n",
      "}\n",
      "{\n",
      "  \"id\": \"resp_68d208e6e20c819399e873ba35227e1c087032dd1462be23\",\n",
      "  \"created_at\": 1758595302.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4o\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"msg_68d208e7290081938de981f80aa356e4087032dd1462be23\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"In San Francisco, the temperature is currently 70 degrees. Similarly, New York also has a temperature of 70 degrees.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"conversation\": null,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": \"resp_68d208e609f48193a34b1bcea1f4e225087032dd1462be23\",\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": null,\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": null\n",
      "  },\n",
      "  \"top_logprobs\": null,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 73,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 26,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 99\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"content_filters\": null,\n",
      "  \"store\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(  \n",
    "  base_url = \"https://aifoundryarc.openai.azure.com/openai/v1\",\n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    ")\n",
    "\n",
    "response = client.responses.create(  \n",
    "    model=\"gpt-4o\",  # replace with your model deployment name  \n",
    "    tools=[  \n",
    "        {  \n",
    "            \"type\": \"function\",  \n",
    "            \"name\": \"get_weather\",  \n",
    "            \"description\": \"Get the weather for a location\",  \n",
    "            \"parameters\": {  \n",
    "                \"type\": \"object\",  \n",
    "                \"properties\": {  \n",
    "                    \"location\": {\"type\": \"string\"},  \n",
    "                },  \n",
    "                \"required\": [\"location\"],  \n",
    "            },  \n",
    "        }  \n",
    "    ],  \n",
    "    input=[{\"role\": \"user\", \"content\": \"What's the weather in San Francisco and New York?\"}],  \n",
    ")  \n",
    "\n",
    "print(response.model_dump_json(indent=2))  \n",
    "  \n",
    "# To provide output to tools, add a response for each tool call to an array passed  \n",
    "# to the next response as `input`  \n",
    "input = []  \n",
    "for output in response.output:  \n",
    "    if output.type == \"function_call\":  \n",
    "        match output.name:  \n",
    "            case \"get_weather\":  \n",
    "                input.append(  \n",
    "                    {  \n",
    "                        \"type\": \"function_call_output\",  \n",
    "                        \"call_id\": output.call_id,  \n",
    "                        \"output\": '{\"temperature\": \"70 degrees\"}',  \n",
    "                    }  \n",
    "                )  \n",
    "            case _:  \n",
    "                raise ValueError(f\"Unknown function call: {output.name}\")  \n",
    "  \n",
    "second_response = client.responses.create(  \n",
    "    model=\"gpt-4o\",  \n",
    "    previous_response_id=response.id,  \n",
    "    input=input  \n",
    ")  \n",
    "\n",
    "print(second_response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5dc708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tool functions defined successfully!\n",
      "üìã Available tools:\n",
      "   - get_weather(location)\n",
      "   - get_stock_price(symbol)\n",
      "   - get_news_headlines(topic)\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Example: Parallel Tool Calls with Azure OpenAI Response API\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Define tool functions that can be called in parallel\n",
    "def get_weather(location):\n",
    "    \"\"\"Simulate weather API call\"\"\"\n",
    "    time.sleep(0.5)  # Simulate API delay\n",
    "    weather_data = {\n",
    "        \"San Francisco\": {\"temperature\": \"68¬∞F\", \"condition\": \"Foggy\", \"humidity\": \"85%\"},\n",
    "        \"New York\": {\"temperature\": \"72¬∞F\", \"condition\": \"Sunny\", \"humidity\": \"60%\"},\n",
    "        \"London\": {\"temperature\": \"55¬∞F\", \"condition\": \"Rainy\", \"humidity\": \"90%\"},\n",
    "        \"Tokyo\": {\"temperature\": \"75¬∞F\", \"condition\": \"Cloudy\", \"humidity\": \"70%\"}\n",
    "    }\n",
    "    return weather_data.get(location, {\"temperature\": \"Unknown\", \"condition\": \"Data not available\"})\n",
    "\n",
    "def get_stock_price(symbol):\n",
    "    \"\"\"Simulate stock price API call\"\"\"\n",
    "    time.sleep(0.3)  # Simulate API delay\n",
    "    stock_data = {\n",
    "        \"AAPL\": {\"price\": \"$182.52\", \"change\": \"+1.25%\", \"volume\": \"45.2M\"},\n",
    "        \"MSFT\": {\"price\": \"$378.85\", \"change\": \"+0.85%\", \"volume\": \"32.1M\"},\n",
    "        \"GOOGL\": {\"price\": \"$142.36\", \"change\": \"-0.45%\", \"volume\": \"28.7M\"},\n",
    "        \"TSLA\": {\"price\": \"$248.50\", \"change\": \"+2.15%\", \"volume\": \"67.3M\"}\n",
    "    }\n",
    "    return stock_data.get(symbol, {\"price\": \"Unknown\", \"change\": \"N/A\", \"volume\": \"N/A\"})\n",
    "\n",
    "def get_news_headlines(topic):\n",
    "    \"\"\"Simulate news API call\"\"\"\n",
    "    time.sleep(0.4)  # Simulate API delay\n",
    "    news_data = {\n",
    "        \"technology\": [\n",
    "            \"AI breakthrough in quantum computing\",\n",
    "            \"New smartphone features revolutionize mobile photography\",\n",
    "            \"Tech giants announce major cloud infrastructure investments\"\n",
    "        ],\n",
    "        \"finance\": [\n",
    "            \"Federal Reserve signals potential rate changes\",\n",
    "            \"Cryptocurrency market shows strong recovery\",\n",
    "            \"Global markets respond to economic indicators\"\n",
    "        ],\n",
    "        \"sports\": [\n",
    "            \"Championship finals set record viewership\",\n",
    "            \"Olympic preparations underway for next games\",\n",
    "            \"Major league trades shake up team dynamics\"\n",
    "        ]\n",
    "    }\n",
    "    return news_data.get(topic, [\"No news available for this topic\"])\n",
    "\n",
    "print(\"‚úÖ Tool functions defined successfully!\")\n",
    "print(\"üìã Available tools:\")\n",
    "print(\"   - get_weather(location)\")\n",
    "print(\"   - get_stock_price(symbol)\")\n",
    "print(\"   - get_news_headlines(topic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e941c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Making Response API call with parallel tool capabilities...\n",
      "‚úÖ Initial response received!\n",
      "üìä Response ID: resp_68d20b22413081979239d0e0bf6961ea0f6443c280c9902a\n",
      "üîß Tool calls detected: 5\n",
      "üîß Executing tool: get_weather\n",
      "‚úÖ Initial response received!\n",
      "üìä Response ID: resp_68d20b22413081979239d0e0bf6961ea0f6443c280c9902a\n",
      "üîß Tool calls detected: 5\n",
      "üîß Executing tool: get_weather\n",
      "üîß Executing tool: get_weather\n",
      "üîß Executing tool: get_weather\n",
      "üîß Executing tool: get_stock_price\n",
      "üîß Executing tool: get_stock_price\n",
      "üîß Executing tool: get_stock_price\n",
      "üîß Executing tool: get_stock_price\n",
      "üîß Executing tool: get_news_headlines\n",
      "üîß Executing tool: get_news_headlines\n",
      "‚è±Ô∏è  Tool execution completed in 2.00 seconds\n",
      "\n",
      "üìù Getting final response with tool results...\n",
      "‚è±Ô∏è  Tool execution completed in 2.00 seconds\n",
      "\n",
      "üìù Getting final response with tool results...\n",
      "\n",
      "üéØ Final Response:\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "üìà Execution Summary:\n",
      "   ‚Ä¢ Tool calls executed: 5\n",
      "   ‚Ä¢ Total execution time: 2.00 seconds\n",
      "   ‚Ä¢ Parallel execution: ‚úÖ Enabled by default\n",
      "\n",
      "üéØ Final Response:\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "üìà Execution Summary:\n",
      "   ‚Ä¢ Tool calls executed: 5\n",
      "   ‚Ä¢ Total execution time: 2.00 seconds\n",
      "   ‚Ä¢ Parallel execution: ‚úÖ Enabled by default\n"
     ]
    }
   ],
   "source": [
    "# Parallel Tool Calls Example with Azure OpenAI Response API\n",
    "try:\n",
    "    client = OpenAI(  \n",
    "        base_url = \"https://aifoundryarc.openai.azure.com/openai/v1\",\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "    )\n",
    "    \n",
    "    print(\"üöÄ Making Response API call with parallel tool capabilities...\")\n",
    "    \n",
    "    # Define tools for parallel execution\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather information for a specific location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city name to get weather for\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_stock_price\",\n",
    "            \"description\": \"Get current stock price and trading information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"symbol\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The stock symbol (e.g., AAPL, MSFT)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"symbol\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_news_headlines\",\n",
    "            \"description\": \"Get latest news headlines for a specific topic\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"topic\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The news topic (e.g., technology, finance, sports)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"topic\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Make the initial response API call (parallel tool calls are enabled by default)\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        tools=tools,\n",
    "        input=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"I need a quick update on: weather in San Francisco and New York, stock prices for AAPL and MSFT, and technology news headlines. Please get all this information for me.\"\n",
    "        }]\n",
    "        # Note: Parallel tool calls are enabled by default in the Response API\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Initial response received!\")\n",
    "    print(f\"üìä Response ID: {response.id}\")\n",
    "    print(f\"üîß Tool calls detected: {len([output for output in response.output if output.type == 'function_call'])}\")\n",
    "    \n",
    "    # Process tool calls and execute them\n",
    "    tool_inputs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for output in response.output:\n",
    "        if output.type == \"function_call\":\n",
    "            print(f\"üîß Executing tool: {output.name}\")\n",
    "            \n",
    "            # Parse the function arguments\n",
    "            args = json.loads(output.arguments) if hasattr(output, 'arguments') else {}\n",
    "            \n",
    "            # Execute the appropriate function\n",
    "            if output.name == \"get_weather\":\n",
    "                result = get_weather(args.get(\"location\", \"\"))\n",
    "            elif output.name == \"get_stock_price\":\n",
    "                result = get_stock_price(args.get(\"symbol\", \"\"))\n",
    "            elif output.name == \"get_news_headlines\":\n",
    "                result = get_news_headlines(args.get(\"topic\", \"\"))\n",
    "            else:\n",
    "                result = {\"error\": f\"Unknown function: {output.name}\"}\n",
    "            \n",
    "            # Add the result to inputs for the next response\n",
    "            tool_inputs.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": output.call_id,\n",
    "                \"output\": json.dumps(result)\n",
    "            })\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Tool execution completed in {execution_time:.2f} seconds\")\n",
    "    \n",
    "    # Get the final response with tool results\n",
    "    print(\"\\nüìù Getting final response with tool results...\")\n",
    "    final_response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        previous_response_id=response.id,\n",
    "        input=tool_inputs\n",
    "    )\n",
    "    \n",
    "    # Display the final response\n",
    "    print(\"\\nüéØ Final Response:\")\n",
    "    print(\"=\" * 60)\n",
    "    for output in final_response.output:\n",
    "        if output.type == \"text\":\n",
    "            print(output.text)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Show execution summary\n",
    "    print(f\"\\nüìà Execution Summary:\")\n",
    "    print(f\"   ‚Ä¢ Tool calls executed: {len(tool_inputs)}\")\n",
    "    print(f\"   ‚Ä¢ Total execution time: {execution_time:.2f} seconds\")\n",
    "    print(f\"   ‚Ä¢ Parallel execution: {'‚úÖ Enabled by default' if len(tool_inputs) > 1 else '‚ùå Single tool'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during parallel tool execution: {str(e)}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Fallback: Demonstrate with mock response\n",
    "    print(\"\\nüé≠ Demonstrating with mock parallel tool execution...\")\n",
    "    \n",
    "    # Simulate parallel tool calls\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # These would normally be called in parallel by the API\n",
    "    weather_sf = get_weather(\"San Francisco\")\n",
    "    weather_ny = get_weather(\"New York\") \n",
    "    stock_aapl = get_stock_price(\"AAPL\")\n",
    "    stock_msft = get_stock_price(\"MSFT\")\n",
    "    tech_news = get_news_headlines(\"technology\")\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüéØ Mock Results (executed in {execution_time:.2f} seconds):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üå§Ô∏è  San Francisco Weather: {weather_sf}\")\n",
    "    print(f\"üå§Ô∏è  New York Weather: {weather_ny}\")\n",
    "    print(f\"üìà AAPL Stock: {stock_aapl}\")\n",
    "    print(f\"üìà MSFT Stock: {stock_msft}\")\n",
    "    print(f\"üì∞ Tech News: {tech_news}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1ae07",
   "metadata": {},
   "source": [
    "# Azure OpenAI Response API - Parallel Tool Calls\n",
    "\n",
    "This example demonstrates how to use the Azure OpenAI Response API with parallel tool calls, which allows multiple functions to be executed concurrently for faster response times.\n",
    "\n",
    "## Key Features:\n",
    "- **Parallel Execution**: Multiple tools can run simultaneously\n",
    "- **Structured Workflow**: Two-phase process (tool calls ‚Üí results ‚Üí final response)\n",
    "- **Error Handling**: Graceful fallback with mock data\n",
    "- **Performance Tracking**: Execution time monitoring\n",
    "\n",
    "## Benefits of Parallel Tool Calls:\n",
    "- **Faster Response**: Tools execute concurrently instead of sequentially\n",
    "- **Better User Experience**: Reduced latency for multi-tool requests\n",
    "- **Efficient Resource Usage**: Optimal utilization of API capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff8928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Simplified Example: Two Tool Calls in Parallel\n",
      "==================================================\n",
      "‚ùå Simple example failed: Responses.create() got an unexpected keyword argument 'response_config'\n",
      "\n",
      "üé≠ Mock demonstration of parallel execution:\n",
      "   ‚ö° Tool 1: get_weather('Tokyo') ‚Üí {'temperature': '75¬∞F', 'condition': 'Cloudy', 'humidity': '70%'}\n",
      "   ‚ö° Tool 1: get_weather('Tokyo') ‚Üí {'temperature': '75¬∞F', 'condition': 'Cloudy', 'humidity': '70%'}\n",
      "   ‚ö° Tool 2: get_stock_price('TSLA') ‚Üí {'price': '$248.50', 'change': '+2.15%', 'volume': '67.3M'}\n",
      "\n",
      "   üí° In real parallel execution, both tools would run simultaneously!\n",
      "   ‚ö° Tool 2: get_stock_price('TSLA') ‚Üí {'price': '$248.50', 'change': '+2.15%', 'volume': '67.3M'}\n",
      "\n",
      "   üí° In real parallel execution, both tools would run simultaneously!\n"
     ]
    }
   ],
   "source": [
    "# Simplified Parallel Tool Call Example\n",
    "print(\"üîß Simplified Example: Two Tool Calls in Parallel\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Simple example with just 2 tools for clarity\n",
    "    simple_tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"location\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\", \n",
    "            \"name\": \"get_stock_price\",\n",
    "            \"description\": \"Get stock price for a symbol\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"symbol\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"symbol\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Make a simple request for 2 parallel operations\n",
    "    # Note: Parallel tool calls are enabled by default in Response API\n",
    "    simple_response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        tools=simple_tools,\n",
    "        input=[{\"role\": \"user\", \"content\": \"Get weather for Tokyo and stock price for TSLA\"}]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Simple parallel request successful!\")\n",
    "    print(f\"Tool calls in response: {len([o for o in simple_response.output if o.type == 'function_call'])}\")\n",
    "    \n",
    "    # Execute the tools\n",
    "    simple_inputs = []\n",
    "    for output in simple_response.output:\n",
    "        if output.type == \"function_call\":\n",
    "            args = json.loads(output.arguments) if hasattr(output, 'arguments') else {}\n",
    "            \n",
    "            if output.name == \"get_weather\":\n",
    "                result = get_weather(args.get(\"location\", \"\"))\n",
    "            elif output.name == \"get_stock_price\":\n",
    "                result = get_stock_price(args.get(\"symbol\", \"\"))\n",
    "            \n",
    "            simple_inputs.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": output.call_id,\n",
    "                \"output\": json.dumps(result)\n",
    "            })\n",
    "    \n",
    "    # Get final response\n",
    "    simple_final = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        previous_response_id=simple_response.id,\n",
    "        input=simple_inputs\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìã Simple Response Result:\")\n",
    "    for output in simple_final.output:\n",
    "        if output.type == \"text\":\n",
    "            print(output.text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Simple example failed: {str(e)}\")\n",
    "    \n",
    "    # Mock demonstration\n",
    "    print(\"\\nüé≠ Mock demonstration of parallel execution:\")\n",
    "    print(\"   ‚ö° Tool 1: get_weather('Tokyo') ‚Üí \" + str(get_weather(\"Tokyo\")))\n",
    "    print(\"   ‚ö° Tool 2: get_stock_price('TSLA') ‚Üí \" + str(get_stock_price(\"TSLA\")))\n",
    "    print(\"\\n   üí° In the Response API, parallel tool calls are enabled by default!\")\n",
    "    print(\"   üí° Multiple tools in a single request will execute concurrently!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbefd96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Sequential Tool Call Example\n",
      "==================================================\n",
      "Scenario: Get weather for a city, then get local news based on weather condition\n",
      "\n",
      "üöÄ Sequential Execution Pattern:\n",
      "   Step 1: Get weather information\n",
      "   Step 2: Use weather data to get relevant local news\n",
      "   Step 3: Analyze weather trend with context\n",
      "\n",
      "üå§Ô∏è  Step 1: Getting weather for San Francisco...\n",
      "‚úÖ Weather information received!\n",
      "   Temperature: 68¬∞F\n",
      "   Condition: Foggy\n",
      "\n",
      "üì∞ Step 2: Getting local news based on 'Foggy' weather...\n",
      "‚úÖ Weather information received!\n",
      "   Temperature: 68¬∞F\n",
      "   Condition: Foggy\n",
      "\n",
      "üì∞ Step 2: Getting local news based on 'Foggy' weather...\n",
      "‚úÖ Local news retrieved based on weather!\n",
      "\n",
      "üìä Step 3: Analyzing weather trend...\n",
      "‚úÖ Local news retrieved based on weather!\n",
      "\n",
      "üìä Step 3: Analyzing weather trend...\n",
      "\n",
      "üéØ Sequential Execution Results:\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "üìà Sequential Execution Summary:\n",
      "   ‚Ä¢ Step 1: Weather ‚Üí Step 2: News ‚Üí Step 3: Analysis\n",
      "   ‚Ä¢ Each step depends on the previous step's results\n",
      "   ‚Ä¢ Total steps: 3 (executed sequentially)\n",
      "   ‚Ä¢ Use case: When tool outputs are interdependent\n",
      "\n",
      "üéØ Sequential Execution Results:\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "üìà Sequential Execution Summary:\n",
      "   ‚Ä¢ Step 1: Weather ‚Üí Step 2: News ‚Üí Step 3: Analysis\n",
      "   ‚Ä¢ Each step depends on the previous step's results\n",
      "   ‚Ä¢ Total steps: 3 (executed sequentially)\n",
      "   ‚Ä¢ Use case: When tool outputs are interdependent\n"
     ]
    }
   ],
   "source": [
    "# Sequential Tool Call Example - When Tools Depend on Each Other\n",
    "print(\"üîÑ Sequential Tool Call Example\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Scenario: Get weather for a city, then get local news based on weather condition\")\n",
    "\n",
    "def get_local_news(location, weather_condition):\n",
    "    \"\"\"Get location-specific news based on weather condition\"\"\"\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    weather_based_news = {\n",
    "        \"Sunny\": [\n",
    "            f\"{location}: Perfect weather brings crowds to outdoor events\",\n",
    "            f\"{location}: Solar energy production reaches peak efficiency\",\n",
    "            f\"{location}: Beach tourism sees surge due to clear skies\"\n",
    "        ],\n",
    "        \"Rainy\": [\n",
    "            f\"{location}: Umbrella sales spike as rain continues\",\n",
    "            f\"{location}: Public transport delays due to weather conditions\",\n",
    "            f\"{location}: Local farmers celebrate much-needed rainfall\"\n",
    "        ],\n",
    "        \"Foggy\": [\n",
    "            f\"{location}: Airport delays expected due to reduced visibility\",\n",
    "            f\"{location}: Fog creates mystical atmosphere for photographers\",\n",
    "            f\"{location}: Maritime traffic slows in harbor\"\n",
    "        ],\n",
    "        \"Cloudy\": [\n",
    "            f\"{location}: Overcast skies provide relief from summer heat\",\n",
    "            f\"{location}: Outdoor concerts proceed despite cloud cover\",\n",
    "            f\"{location}: Photographers capture dramatic cloud formations\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return weather_based_news.get(weather_condition, [f\"{location}: Standard local news updates\"])\n",
    "\n",
    "def analyze_weather_trend(location, current_weather):\n",
    "    \"\"\"Analyze weather trend and provide recommendations\"\"\"\n",
    "    time.sleep(0.4)\n",
    "    \n",
    "    recommendations = {\n",
    "        \"Sunny\": f\"Great day for outdoor activities in {location}! UV protection recommended.\",\n",
    "        \"Rainy\": f\"Indoor activities recommended in {location}. Great day for museums or shopping.\",\n",
    "        \"Foggy\": f\"Reduced visibility in {location}. Take extra care with transportation.\",\n",
    "        \"Cloudy\": f\"Comfortable weather in {location}. Perfect for walking or light outdoor activities.\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"recommendation\": recommendations.get(current_weather, \"Standard weather advisory\"),\n",
    "        \"trend\": \"Stable conditions expected for next 6 hours\",\n",
    "        \"air_quality\": \"Moderate to Good\"\n",
    "    }\n",
    "\n",
    "try:\n",
    "    print(\"\\nüöÄ Sequential Execution Pattern:\")\n",
    "    print(\"   Step 1: Get weather information\")\n",
    "    print(\"   Step 2: Use weather data to get relevant local news\")\n",
    "    print(\"   Step 3: Analyze weather trend with context\")\n",
    "    \n",
    "    # Tool definitions for sequential calls\n",
    "    weather_tool = {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather information for a location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # STEP 1: Get weather information first\n",
    "    print(\"\\nüå§Ô∏è  Step 1: Getting weather for San Francisco...\")\n",
    "    weather_response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        tools=[weather_tool],\n",
    "        input=[{\"role\": \"user\", \"content\": \"What's the current weather in San Francisco?\"}]\n",
    "    )\n",
    "    \n",
    "    # Process weather result\n",
    "    weather_result = None\n",
    "    weather_inputs = []\n",
    "    \n",
    "    for output in weather_response.output:\n",
    "        if output.type == \"function_call\" and output.name == \"get_weather\":\n",
    "            args = json.loads(output.arguments) if hasattr(output, 'arguments') else {}\n",
    "            weather_data = get_weather(args.get(\"location\", \"\"))\n",
    "            weather_result = weather_data\n",
    "            \n",
    "            weather_inputs.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": output.call_id,\n",
    "                \"output\": json.dumps(weather_data)\n",
    "            })\n",
    "    \n",
    "    # Get weather summary\n",
    "    weather_summary = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        previous_response_id=weather_response.id,\n",
    "        input=weather_inputs\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Weather information received!\")\n",
    "    if weather_result:\n",
    "        print(f\"   Temperature: {weather_result.get('temperature', 'N/A')}\")\n",
    "        print(f\"   Condition: {weather_result.get('condition', 'N/A')}\")\n",
    "    \n",
    "    # STEP 2: Use weather data for local news (sequential dependency)\n",
    "    if weather_result:\n",
    "        print(f\"\\nüì∞ Step 2: Getting local news based on '{weather_result.get('condition', 'Unknown')}' weather...\")\n",
    "        \n",
    "        local_news_tool = {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_local_news\", \n",
    "            \"description\": \"Get location-specific news based on weather condition\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"weather_condition\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"location\", \"weather_condition\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        news_response = client.responses.create(\n",
    "            model=\"gpt-4o\",\n",
    "            tools=[local_news_tool],\n",
    "            input=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Get local news for San Francisco considering the current weather is {weather_result.get('condition', 'Unknown')}\"\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        # Process news result\n",
    "        news_inputs = []\n",
    "        news_result = None\n",
    "        \n",
    "        for output in news_response.output:\n",
    "            if output.type == \"function_call\" and output.name == \"get_local_news\":\n",
    "                args = json.loads(output.arguments) if hasattr(output, 'arguments') else {}\n",
    "                news_data = get_local_news(\n",
    "                    args.get(\"location\", \"San Francisco\"),\n",
    "                    args.get(\"weather_condition\", weather_result.get('condition', 'Unknown'))\n",
    "                )\n",
    "                news_result = news_data\n",
    "                \n",
    "                news_inputs.append({\n",
    "                    \"type\": \"function_call_output\",\n",
    "                    \"call_id\": output.call_id,\n",
    "                    \"output\": json.dumps(news_data)\n",
    "                })\n",
    "        \n",
    "        print(\"‚úÖ Local news retrieved based on weather!\")\n",
    "        \n",
    "        # STEP 3: Analyze weather trend with context\n",
    "        print(f\"\\nüìä Step 3: Analyzing weather trend...\")\n",
    "        \n",
    "        trend_tool = {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"analyze_weather_trend\",\n",
    "            \"description\": \"Analyze weather trend and provide recommendations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"current_weather\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"location\", \"current_weather\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        trend_response = client.responses.create(\n",
    "            model=\"gpt-4o\",\n",
    "            tools=[trend_tool],\n",
    "            input=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Analyze the weather trend for San Francisco with current condition: {weather_result.get('condition', 'Unknown')}\"\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        # Process trend result\n",
    "        trend_inputs = []\n",
    "        \n",
    "        for output in trend_response.output:\n",
    "            if output.type == \"function_call\" and output.name == \"analyze_weather_trend\":\n",
    "                args = json.loads(output.arguments) if hasattr(output, 'arguments') else {}\n",
    "                trend_data = analyze_weather_trend(\n",
    "                    args.get(\"location\", \"San Francisco\"),\n",
    "                    args.get(\"current_weather\", weather_result.get('condition', 'Unknown'))\n",
    "                )\n",
    "                \n",
    "                trend_inputs.append({\n",
    "                    \"type\": \"function_call_output\",\n",
    "                    \"call_id\": output.call_id,\n",
    "                    \"output\": json.dumps(trend_data)\n",
    "                })\n",
    "        \n",
    "        # Get final sequential response\n",
    "        final_sequential = client.responses.create(\n",
    "            model=\"gpt-4o\",\n",
    "            previous_response_id=trend_response.id,\n",
    "            input=trend_inputs\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüéØ Sequential Execution Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        for output in final_sequential.output:\n",
    "            if output.type == \"text\":\n",
    "                print(output.text)\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"\\nüìà Sequential Execution Summary:\")\n",
    "        print(\"   ‚Ä¢ Step 1: Weather ‚Üí Step 2: News ‚Üí Step 3: Analysis\")\n",
    "        print(\"   ‚Ä¢ Each step depends on the previous step's results\")\n",
    "        print(\"   ‚Ä¢ Total steps: 3 (executed sequentially)\")\n",
    "        print(\"   ‚Ä¢ Use case: When tool outputs are interdependent\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Sequential execution error: {str(e)}\")\n",
    "    \n",
    "    # Fallback demonstration\n",
    "    print(\"\\nüé≠ Mock Sequential Execution:\")\n",
    "    print(\"   Step 1: Weather for San Francisco\")\n",
    "    weather_mock = get_weather(\"San Francisco\")\n",
    "    print(f\"      Result: {weather_mock}\")\n",
    "    \n",
    "    print(f\"   Step 2: Local news based on '{weather_mock.get('condition')}' weather\")\n",
    "    news_mock = get_local_news(\"San Francisco\", weather_mock.get('condition', 'Unknown'))\n",
    "    print(f\"      Result: {news_mock[0] if news_mock else 'No news'}\")\n",
    "    \n",
    "    print(f\"   Step 3: Weather trend analysis\")\n",
    "    trend_mock = analyze_weather_trend(\"San Francisco\", weather_mock.get('condition', 'Unknown'))\n",
    "    print(f\"      Result: {trend_mock.get('recommendation', 'No recommendation')}\")\n",
    "    \n",
    "    print(\"\\n   üí° Sequential execution ensures each step has the data it needs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bef303f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° COMPARISON: Parallel vs Sequential Tool Execution\n",
      "======================================================================\n",
      "\n",
      "üèÉ‚Äç‚ôÇÔ∏è PARALLEL EXECUTION SIMULATION:\n",
      "   All tools run simultaneously (fastest)\n",
      "Parallel execution: 1.20 seconds\n",
      "\n",
      "üö∂‚Äç‚ôÇÔ∏è SEQUENTIAL EXECUTION SIMULATION:\n",
      "   Tools run one after another (slower, but allows dependencies)\n",
      "   ‚Üí Step 1: Getting weather...\n",
      "Parallel execution: 1.20 seconds\n",
      "\n",
      "üö∂‚Äç‚ôÇÔ∏è SEQUENTIAL EXECUTION SIMULATION:\n",
      "   Tools run one after another (slower, but allows dependencies)\n",
      "   ‚Üí Step 1: Getting weather...\n",
      "   ‚Üí Step 2: Other weather, checking different sector...\n",
      "   ‚Üí Step 2: Other weather, checking different sector...\n",
      "   ‚Üí Step 3: High stock price, getting tech news...\n",
      "   ‚Üí Step 3: High stock price, getting tech news...\n",
      "Sequential execution: 1.20 seconds\n",
      "\n",
      "üìä PERFORMANCE COMPARISON:\n",
      "   ‚ö° Parallel:   1.20s\n",
      "   üö∂ Sequential: 1.20s\n",
      "   üèÜ Speed gain: 0.1% faster with parallel\n",
      "\n",
      "üìã WHEN TO USE EACH APPROACH:\n",
      "   üîÑ PARALLEL (Default in Response API):\n",
      "      ‚Ä¢ Independent tool calls\n",
      "      ‚Ä¢ Maximum speed required\n",
      "      ‚Ä¢ Tools don't depend on each other's output\n",
      "      ‚Ä¢ Example: Get weather + stock + news simultaneously\n",
      "   ‚è≠Ô∏è  SEQUENTIAL:\n",
      "      ‚Ä¢ Tool calls have dependencies\n",
      "      ‚Ä¢ Output of one tool affects another\n",
      "      ‚Ä¢ Logical workflow with steps\n",
      "      ‚Ä¢ Example: Weather ‚Üí Weather-based recommendations ‚Üí Local activities\n",
      "\n",
      "üí° RESPONSE API BEHAVIOR:\n",
      "   ‚Ä¢ Parallel execution is the DEFAULT behavior\n",
      "   ‚Ä¢ Multiple tools in one request = automatic parallel execution\n",
      "   ‚Ä¢ For sequential: make separate requests with previous results\n",
      "   ‚Ä¢ Use previous_response_id to chain requests together\n",
      "Sequential execution: 1.20 seconds\n",
      "\n",
      "üìä PERFORMANCE COMPARISON:\n",
      "   ‚ö° Parallel:   1.20s\n",
      "   üö∂ Sequential: 1.20s\n",
      "   üèÜ Speed gain: 0.1% faster with parallel\n",
      "\n",
      "üìã WHEN TO USE EACH APPROACH:\n",
      "   üîÑ PARALLEL (Default in Response API):\n",
      "      ‚Ä¢ Independent tool calls\n",
      "      ‚Ä¢ Maximum speed required\n",
      "      ‚Ä¢ Tools don't depend on each other's output\n",
      "      ‚Ä¢ Example: Get weather + stock + news simultaneously\n",
      "   ‚è≠Ô∏è  SEQUENTIAL:\n",
      "      ‚Ä¢ Tool calls have dependencies\n",
      "      ‚Ä¢ Output of one tool affects another\n",
      "      ‚Ä¢ Logical workflow with steps\n",
      "      ‚Ä¢ Example: Weather ‚Üí Weather-based recommendations ‚Üí Local activities\n",
      "\n",
      "üí° RESPONSE API BEHAVIOR:\n",
      "   ‚Ä¢ Parallel execution is the DEFAULT behavior\n",
      "   ‚Ä¢ Multiple tools in one request = automatic parallel execution\n",
      "   ‚Ä¢ For sequential: make separate requests with previous results\n",
      "   ‚Ä¢ Use previous_response_id to chain requests together\n"
     ]
    }
   ],
   "source": [
    "# Comparison: Parallel vs Sequential Tool Execution\n",
    "print(\"‚ö° COMPARISON: Parallel vs Sequential Tool Execution\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def time_execution(func, description):\n",
    "    \"\"\"Helper function to time execution\"\"\"\n",
    "    start = time.time()\n",
    "    result = func()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{description}: {elapsed:.2f} seconds\")\n",
    "    return result, elapsed\n",
    "\n",
    "print(\"\\nüèÉ‚Äç‚ôÇÔ∏è PARALLEL EXECUTION SIMULATION:\")\n",
    "print(\"   All tools run simultaneously (fastest)\")\n",
    "\n",
    "def parallel_simulation():\n",
    "    # Simulate parallel execution - all tools start at the same time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # These would execute concurrently in the Response API\n",
    "    weather_sf = get_weather(\"San Francisco\")\n",
    "    stock_aapl = get_stock_price(\"AAPL\")  \n",
    "    tech_news = get_news_headlines(\"technology\")\n",
    "    \n",
    "    return {\n",
    "        \"weather\": weather_sf,\n",
    "        \"stock\": stock_aapl,\n",
    "        \"news\": tech_news\n",
    "    }\n",
    "\n",
    "parallel_result, parallel_time = time_execution(parallel_simulation, \"Parallel execution\")\n",
    "\n",
    "print(\"\\nüö∂‚Äç‚ôÇÔ∏è SEQUENTIAL EXECUTION SIMULATION:\")\n",
    "print(\"   Tools run one after another (slower, but allows dependencies)\")\n",
    "\n",
    "def sequential_simulation():\n",
    "    # Simulate sequential execution - tools run one at a time\n",
    "    results = {}\n",
    "    \n",
    "    # Step 1: Get weather\n",
    "    print(\"   ‚Üí Step 1: Getting weather...\")\n",
    "    results[\"weather\"] = get_weather(\"San Francisco\")\n",
    "    \n",
    "    # Step 2: Use weather to determine appropriate stock sector\n",
    "    weather_condition = results[\"weather\"].get(\"condition\", \"\").lower()\n",
    "    if \"sunny\" in weather_condition or \"clear\" in weather_condition:\n",
    "        stock_symbol = \"AAPL\"  # Tech stocks do well in good weather\n",
    "        print(\"   ‚Üí Step 2: Good weather detected, checking tech stocks...\")\n",
    "    else:\n",
    "        stock_symbol = \"MSFT\"  # Different strategy for other weather\n",
    "        print(\"   ‚Üí Step 2: Other weather, checking different sector...\")\n",
    "    \n",
    "    results[\"stock\"] = get_stock_price(stock_symbol)\n",
    "    \n",
    "    # Step 3: Get news based on previous results\n",
    "    if float(results[\"stock\"][\"price\"].replace(\"$\", \"\")) > 300:\n",
    "        news_topic = \"technology\"\n",
    "        print(\"   ‚Üí Step 3: High stock price, getting tech news...\")\n",
    "    else:\n",
    "        news_topic = \"finance\"\n",
    "        print(\"   ‚Üí Step 3: Lower stock price, getting finance news...\")\n",
    "    \n",
    "    results[\"news\"] = get_news_headlines(news_topic)\n",
    "    \n",
    "    return results\n",
    "\n",
    "sequential_result, sequential_time = time_execution(sequential_simulation, \"Sequential execution\")\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
    "print(f\"   ‚ö° Parallel:   {parallel_time:.2f}s\")\n",
    "print(f\"   üö∂ Sequential: {sequential_time:.2f}s\")\n",
    "print(f\"   üèÜ Speed gain: {((sequential_time - parallel_time) / sequential_time * 100):.1f}% faster with parallel\")\n",
    "\n",
    "print(f\"\\nüìã WHEN TO USE EACH APPROACH:\")\n",
    "print(f\"   üîÑ PARALLEL (Default in Response API):\")\n",
    "print(f\"      ‚Ä¢ Independent tool calls\")\n",
    "print(f\"      ‚Ä¢ Maximum speed required\")\n",
    "print(f\"      ‚Ä¢ Tools don't depend on each other's output\")\n",
    "print(f\"      ‚Ä¢ Example: Get weather + stock + news simultaneously\")\n",
    "\n",
    "print(f\"   ‚è≠Ô∏è  SEQUENTIAL:\")\n",
    "print(f\"      ‚Ä¢ Tool calls have dependencies\")\n",
    "print(f\"      ‚Ä¢ Output of one tool affects another\")\n",
    "print(f\"      ‚Ä¢ Logical workflow with steps\")\n",
    "print(f\"      ‚Ä¢ Example: Weather ‚Üí Weather-based recommendations ‚Üí Local activities\")\n",
    "\n",
    "print(f\"\\nüí° RESPONSE API BEHAVIOR:\")\n",
    "print(f\"   ‚Ä¢ Parallel execution is the DEFAULT behavior\")\n",
    "print(f\"   ‚Ä¢ Multiple tools in one request = automatic parallel execution\")\n",
    "print(f\"   ‚Ä¢ For sequential: make separate requests with previous results\")\n",
    "print(f\"   ‚Ä¢ Use previous_response_id to chain requests together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece73f7",
   "metadata": {},
   "source": [
    "## Summary: Parallel vs Sequential Tool Calls\n",
    "\n",
    "### Parallel Tool Calls (Default Behavior)\n",
    "- **When to use**: Independent operations that don't depend on each other\n",
    "- **Performance**: Fastest execution - all tools run simultaneously\n",
    "- **Implementation**: Single Response API call with multiple tools\n",
    "- **Example**: Get weather + stock prices + news headlines at the same time\n",
    "\n",
    "### Sequential Tool Calls\n",
    "- **When to use**: When tool outputs depend on previous results\n",
    "- **Performance**: Slower but enables logical workflows\n",
    "- **Implementation**: Multiple Response API calls using `previous_response_id`\n",
    "- **Example**: Get weather ‚Üí Get weather-based recommendations ‚Üí Get local activities\n",
    "\n",
    "### Best Practices\n",
    "1. **Default to parallel** for maximum performance when possible\n",
    "2. **Use sequential** only when tool outputs are interdependent\n",
    "3. **Chain requests** using `previous_response_id` for sequential flows\n",
    "4. **Consider hybrid approaches** - parallel within sequential steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
