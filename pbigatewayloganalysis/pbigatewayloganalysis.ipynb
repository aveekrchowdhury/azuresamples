{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f504abd3",
   "metadata": {},
   "source": [
    "# Power BI Gateway Log Analysis\n",
    "\n",
    "This notebook analyzes Power BI Gateway logs, specifically focusing on:\n",
    "- Query Execution Reports\n",
    "- Query Start Reports\n",
    "\n",
    "The logs are loaded from the path specified in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d8acee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e4e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file path from .env: C:\\Users\u0007veekr\\OneDrive - Microsoft\\Documents\\Customers\\NFM\\OneDrive_1_9-15-2025\n",
      "WARNING: Path does not exist: C:\\Users\u0007veekr\\OneDrive - Microsoft\\Documents\\Customers\\NFM\\OneDrive_1_9-15-2025\n",
      "This could happen if:\n",
      "1. The path in .env file is incorrect\n",
      "2. The path is on a different machine or network drive\n",
      "3. The path needs to be mapped or accessible\n",
      "\n",
      "Creating sample directory structure at: c:\\Users\\aveekr\\OneDrive - Microsoft\\Documents\\Demos\\my_code_samples\\azuresamples\\sample_gateway_logs\n",
      "Sample files created successfully!\n",
      "Using sample path: c:\\Users\\aveekr\\OneDrive - Microsoft\\Documents\\Demos\\my_code_samples\\azuresamples\\sample_gateway_logs\n",
      "Found 2 folders: ['Gateway1', 'Gateway2']\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Load path from .env file\n",
    "log_file_path = os.getenv('LOG_FILE_PATH')\n",
    "if not log_file_path:\n",
    "    raise ValueError(\"LOG_FILE_PATH not found in .env file\")\n",
    "\n",
    "print(f\"Log file path from .env: {log_file_path}\")\n",
    "\n",
    "# Check if path exists and handle gracefully\n",
    "if not os.path.exists(log_file_path):\n",
    "    print(f\"WARNING: Path does not exist: {log_file_path}\")\n",
    "    print(\"This could happen if:\")\n",
    "    print(\"1. The path in .env file is incorrect\")\n",
    "    print(\"2. The path is on a different machine or network drive\")\n",
    "    print(\"3. The path needs to be mapped or accessible\")\n",
    "    \n",
    "    # Create a sample directory structure for demonstration\n",
    "    current_dir = os.path.dirname(os.path.abspath(\"\"))\n",
    "    sample_path = os.path.join(current_dir, \"sample_gateway_logs\")\n",
    "    \n",
    "    print(f\"\\nCreating sample directory structure at: {sample_path}\")\n",
    "    \n",
    "    # Create sample directories\n",
    "    os.makedirs(os.path.join(sample_path, \"Gateway1\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(sample_path, \"Gateway2\"), exist_ok=True)\n",
    "    \n",
    "    # Create sample CSV files\n",
    "    sample_execution_data = {\n",
    "        'RequestId': ['req1', 'req2', 'req3'],\n",
    "        'StartTime': ['2025-09-19 10:00:00', '2025-09-19 10:01:00', '2025-09-19 10:02:00'],\n",
    "        'EndTime': ['2025-09-19 10:00:05', '2025-09-19 10:01:03', '2025-09-19 10:02:02'],\n",
    "        'Duration': [5000, 3000, 2000],\n",
    "        'Status': ['Success', 'Success', 'Failed']\n",
    "    }\n",
    "    \n",
    "    sample_start_data = {\n",
    "        'RequestId': ['req1', 'req2', 'req3'],\n",
    "        'StartTime': ['2025-09-19 10:00:00', '2025-09-19 10:01:00', '2025-09-19 10:02:00'],\n",
    "        'QueryType': ['DirectQuery', 'Import', 'DirectQuery'],\n",
    "        'DataSource': ['SQL Server', 'SharePoint', 'SQL Server']\n",
    "    }\n",
    "    \n",
    "    # Save sample files\n",
    "    pd.DataFrame(sample_execution_data).to_csv(\n",
    "        os.path.join(sample_path, \"Gateway1\", \"QueryExecutionReport_20250919.csv\"), \n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame(sample_start_data).to_csv(\n",
    "        os.path.join(sample_path, \"Gateway1\", \"QueryStartReport_20250919.csv\"), \n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame(sample_execution_data).to_csv(\n",
    "        os.path.join(sample_path, \"Gateway2\", \"QueryExecutionReport_20250919.csv\"), \n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame(sample_start_data).to_csv(\n",
    "        os.path.join(sample_path, \"Gateway2\", \"QueryStartReport_20250919.csv\"), \n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    print(\"Sample files created successfully!\")\n",
    "    log_file_path = sample_path\n",
    "    print(f\"Using sample path: {log_file_path}\")\n",
    "\n",
    "# List all folders in the path\n",
    "folders = [f for f in os.listdir(log_file_path) if os.path.isdir(os.path.join(log_file_path, f))]\n",
    "print(f\"Found {len(folders)} folders: {folders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2a1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_query_execution_reports(folder_path):\n",
    "    \"\"\"\n",
    "    Load Query Execution Report files from a given folder.\n",
    "    These files typically contain information about completed queries.\n",
    "    \"\"\"\n",
    "    query_execution_files = []\n",
    "    execution_data = []\n",
    "    \n",
    "    # Common patterns for query execution report files\n",
    "    execution_patterns = ['*QueryExecutionReport*']\n",
    "    \n",
    "    for pattern in execution_patterns:\n",
    "        files = glob.glob(os.path.join(folder_path, pattern))\n",
    "        query_execution_files.extend(files)\n",
    "    \n",
    "    print(f\"Found {len(query_execution_files)} query execution files in {folder_path}\")\n",
    "    \n",
    "    for file_path in query_execution_files:\n",
    "        try:\n",
    "            if file_path.lower().endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                df['folder'] = os.path.basename(folder_path)\n",
    "                execution_data.append(df)\n",
    "                print(f\"  Loaded CSV: {os.path.basename(file_path)} ({len(df)} rows)\")\n",
    "            elif file_path.lower().endswith('.json'):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                df = pd.json_normalize(json_data)\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                df['folder'] = os.path.basename(folder_path)\n",
    "                execution_data.append(df)\n",
    "                print(f\"  Loaded JSON: {os.path.basename(file_path)} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    return execution_data\n",
    "\n",
    "def load_query_start_reports(folder_path):\n",
    "    \"\"\"\n",
    "    Load Query Start Report files from a given folder.\n",
    "    These files typically contain information about query initiation.\n",
    "    \"\"\"\n",
    "    query_start_files = []\n",
    "    start_data = []\n",
    "    \n",
    "    # Common patterns for query start report files\n",
    "    start_patterns = [\n",
    "        '*QueryStartReport*'\n",
    "    ]\n",
    "    \n",
    "    for pattern in start_patterns:\n",
    "        files = glob.glob(os.path.join(folder_path, pattern))\n",
    "        query_start_files.extend(files)\n",
    "    \n",
    "    print(f\"Found {len(query_start_files)} query start files in {folder_path}\")\n",
    "    \n",
    "    for file_path in query_start_files:\n",
    "        try:\n",
    "            if file_path.lower().endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                df['folder'] = os.path.basename(folder_path)\n",
    "                start_data.append(df)\n",
    "                print(f\"  Loaded CSV: {os.path.basename(file_path)} ({len(df)} rows)\")\n",
    "            elif file_path.lower().endswith('.json'):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                df = pd.json_normalize(json_data)\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                df['folder'] = os.path.basename(folder_path)\n",
    "                start_data.append(df)\n",
    "                print(f\"  Loaded JSON: {os.path.basename(file_path)} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    return start_data\n",
    "\n",
    "print(\"Data loading functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data loading script\n",
    "all_execution_data = []\n",
    "all_start_data = []\n",
    "\n",
    "print(\"Starting data loading process...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Process each folder in the log path\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(log_file_path, folder)\n",
    "    print(f\"\\nProcessing folder: {folder}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Load query execution reports from this folder\n",
    "    execution_data = load_query_execution_reports(folder_path)\n",
    "    all_execution_data.extend(execution_data)\n",
    "    \n",
    "    # Load query start reports from this folder\n",
    "    start_data = load_query_start_reports(folder_path)\n",
    "    all_start_data.extend(start_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Data loading summary:\")\n",
    "print(f\"Total execution report datasets: {len(all_execution_data)}\")\n",
    "print(f\"Total start report datasets: {len(all_start_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b78d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all execution data into single DataFrames\n",
    "if all_execution_data:\n",
    "    execution_df = pd.concat(all_execution_data, ignore_index=True)\n",
    "    print(f\"\\nCombined Query Execution Report:\")\n",
    "    print(f\"Total rows: {len(execution_df)}\")\n",
    "    print(f\"Columns: {list(execution_df.columns)}\")\n",
    "    print(f\"Data sources: {execution_df['source_file'].nunique()} files from {execution_df['folder'].nunique()} folders\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows of execution data:\")\n",
    "    display(execution_df.head())\n",
    "else:\n",
    "    execution_df = pd.DataFrame()\n",
    "    print(\"\\nNo query execution report data found\")\n",
    "\n",
    "if all_start_data:\n",
    "    start_df = pd.concat(all_start_data, ignore_index=True)\n",
    "    print(f\"\\nCombined Query Start Report:\")\n",
    "    print(f\"Total rows: {len(start_df)}\")\n",
    "    print(f\"Columns: {list(start_df.columns)}\")\n",
    "    print(f\"Data sources: {start_df['source_file'].nunique()} files from {start_df['folder'].nunique()} folders\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows of start data:\")\n",
    "    display(start_df.head())\n",
    "else:\n",
    "    start_df = pd.DataFrame()\n",
    "    print(\"\\nNo query start report data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6145fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save combined data to CSV files for further analysis\n",
    "save_to_csv = True  # Set to False if you don't want to save\n",
    "\n",
    "if save_to_csv:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if not execution_df.empty:\n",
    "        execution_filename = f\"combined_query_execution_report_{timestamp}.csv\"\n",
    "        execution_df.to_csv(execution_filename, index=False)\n",
    "        print(f\"Query execution data saved to: {execution_filename}\")\n",
    "    \n",
    "    if not start_df.empty:\n",
    "        start_filename = f\"combined_query_start_report_{timestamp}.csv\"\n",
    "        start_df.to_csv(start_filename, index=False)\n",
    "        print(f\"Query start data saved to: {start_filename}\")\n",
    "\n",
    "print(\"\\nData loading complete! You can now analyze the data using the variables:\")\n",
    "print(\"- execution_df: Contains all query execution report data\")\n",
    "print(\"- start_df: Contains all query start report data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
