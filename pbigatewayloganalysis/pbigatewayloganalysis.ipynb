{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f504abd3",
   "metadata": {},
   "source": [
    "# Power BI Gateway Log Analysis\n",
    "\n",
    "This notebook analyzes Power BI Gateway logs, specifically focusing on:\n",
    "- Query Execution Reports\n",
    "- Query Start Reports\n",
    "\n",
    "The logs are loaded from the path specified in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d8acee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e4e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file path from .env: C:\\Users\u0007veekr\\OneDrive - Microsoft\\Documents\\Customers\\NFM\\OneDrive_1_9-15-2025\n",
      "WARNING: Path does not exist: C:\\Users\u0007veekr\\OneDrive - Microsoft\\Documents\\Customers\\NFM\\OneDrive_1_9-15-2025\n",
      "This could happen if:\n",
      "1. The path in .env file is incorrect\n",
      "2. The path is on a different machine or network drive\n",
      "3. The path needs to be mapped or accessible\n",
      "\n",
      "Creating sample directory structure at: c:\\Users\\aveekr\\OneDrive - Microsoft\\Documents\\Demos\\my_code_samples\\azuresamples\\sample_gateway_logs\n",
      "Sample files created successfully!\n",
      "Using sample path: c:\\Users\\aveekr\\OneDrive - Microsoft\\Documents\\Demos\\my_code_samples\\azuresamples\\sample_gateway_logs\n",
      "Found 2 folders: ['Gateway1', 'Gateway2']\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Load path from .env file\n",
    "log_file_path = os.getenv('LOG_FILE_PATH')\n",
    "if not log_file_path:\n",
    "    raise ValueError(\"LOG_FILE_PATH not found in .env file\")\n",
    "\n",
    "print(f\"Log file path from .env: {log_file_path}\")\n",
    "\n",
    "# Check if path exists and handle gracefully\n",
    "if not os.path.exists(log_file_path):\n",
    "    print(f\"WARNING: Path does not exist: {log_file_path}\")\n",
    "    print(\"This could happen if:\")\n",
    "    print(\"1. The path in .env file is incorrect\")\n",
    "    print(\"2. The path is on a different machine or network drive\")\n",
    "    print(\"3. The path needs to be mapped or accessible\")\n",
    "    \n",
    "    # Create a sample directory structure for demonstration\n",
    "    current_dir = os.path.dirname(os.path.abspath(\"\"))\n",
    "    sample_path = os.path.join(current_dir, \"sample_gateway_logs\")\n",
    "    \n",
    "    print(f\"\\nCreating sample directory structure at: {sample_path}\")\n",
    "    \n",
    "    # Create sample directories\n",
    "    os.makedirs(os.path.join(sample_path, \"Gateway1\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(sample_path, \"Gateway2\"), exist_ok=True)\n",
    "    \n",
    "    # Create sample CSV files\n",
    "    sample_execution_data = {\n",
    "        'RequestId': ['req1', 'req2', 'req3'],\n",
    "        'StartTime': ['2025-09-19 10:00:00', '2025-09-19 10:01:00', '2025-09-19 10:02:00'],\n",
    "        'EndTime': ['2025-09-19 10:00:05', '2025-09-19 10:01:03', '2025-09-19 10:02:02'],\n",
    "        'Duration': [5000, 3000, 2000],\n",
    "        'Status': ['Success', 'Success', 'Failed']\n",
    "    }\n",
    "    \n",
    "    sample_start_data = {\n",
    "        'RequestId': ['req1', 'req2', 'req3'],\n",
    "        'StartTime': ['2025-09-19 10:00:00', '2025-09-19 10:01:00', '2025-09-19 10:02:00'],\n",
    "        'QueryType': ['DirectQuery', 'Import', 'DirectQuery'],\n",
    "        'DataSource': ['SQL Server', 'SharePoint', 'SQL Server']\n",
    "    }\n",
    "    \n",
    "    # Save sample files\n",
    "    pd.DataFrame(sample_execution_data).to_csv(\n",
    "        os.path.join(sample_path, \"Gateway1\", \"QueryExecutionReport_20250919.csv\"), \n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame(sample_start_data).to_csv(\n",
    "        os.path.join(sample_path, \"Gateway1\", \"QueryStartReport_20250919.csv\"), \n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame(sample_execution_data).to_csv(\n",
    "        os.path.join(sample_path, \"Gateway2\", \"QueryExecutionReport_20250919.csv\"), \n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame(sample_start_data).to_csv(\n",
    "        os.path.join(sample_path, \"Gateway2\", \"QueryStartReport_20250919.csv\"), \n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    print(\"Sample files created successfully!\")\n",
    "    log_file_path = sample_path\n",
    "    print(f\"Using sample path: {log_file_path}\")\n",
    "\n",
    "# List all folders in the path\n",
    "folders = [f for f in os.listdir(log_file_path) if os.path.isdir(os.path.join(log_file_path, f))]\n",
    "print(f\"Found {len(folders)} folders: {folders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2a1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_query_execution_reports(folder_path):\n",
    "    \"\"\"\n",
    "    Load Query Execution Report files from a given folder.\n",
    "    These files typically contain information about completed queries.\n",
    "    \"\"\"\n",
    "    query_execution_files = []\n",
    "    execution_data = []\n",
    "    \n",
    "    # Common patterns for query execution report files\n",
    "    execution_patterns = ['*QueryExecutionReport*']\n",
    "    \n",
    "    for pattern in execution_patterns:\n",
    "        files = glob.glob(os.path.join(folder_path, pattern))\n",
    "        query_execution_files.extend(files)\n",
    "    \n",
    "    print(f\"Found {len(query_execution_files)} query execution files in {folder_path}\")\n",
    "    \n",
    "    for file_path in query_execution_files:\n",
    "        try:\n",
    "            if file_path.lower().endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                df['folder'] = os.path.basename(folder_path)\n",
    "                execution_data.append(df)\n",
    "                print(f\"  Loaded CSV: {os.path.basename(file_path)} ({len(df)} rows)\")\n",
    "            elif file_path.lower().endswith('.json'):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                df = pd.json_normalize(json_data)\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                df['folder'] = os.path.basename(folder_path)\n",
    "                execution_data.append(df)\n",
    "                print(f\"  Loaded JSON: {os.path.basename(file_path)} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    return execution_data\n",
    "\n",
    "def load_query_start_reports(folder_path):\n",
    "    \"\"\"\n",
    "    Load Query Start Report files from a given folder.\n",
    "    These files typically contain information about query initiation.\n",
    "    \"\"\"\n",
    "    query_start_files = []\n",
    "    start_data = []\n",
    "    \n",
    "    # Common patterns for query start report files\n",
    "    start_patterns = [\n",
    "        '*QueryStartReport*'\n",
    "    ]\n",
    "    \n",
    "    for pattern in start_patterns:\n",
    "        files = glob.glob(os.path.join(folder_path, pattern))\n",
    "        query_start_files.extend(files)\n",
    "    \n",
    "    print(f\"Found {len(query_start_files)} query start files in {folder_path}\")\n",
    "    \n",
    "    for file_path in query_start_files:\n",
    "        try:\n",
    "            if file_path.lower().endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                df['folder'] = os.path.basename(folder_path)\n",
    "                start_data.append(df)\n",
    "                print(f\"  Loaded CSV: {os.path.basename(file_path)} ({len(df)} rows)\")\n",
    "            elif file_path.lower().endswith('.json'):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                df = pd.json_normalize(json_data)\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                df['folder'] = os.path.basename(folder_path)\n",
    "                start_data.append(df)\n",
    "                print(f\"  Loaded JSON: {os.path.basename(file_path)} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    return start_data\n",
    "\n",
    "print(\"Data loading functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6359431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading process...\n",
      "==================================================\n",
      "\n",
      "Processing folder: Gateway1\n",
      "------------------------------\n",
      "Found 1 query execution files in c:\\Users\\aveekr\\OneDrive - Microsoft\\Documents\\Demos\\my_code_samples\\azuresamples\\sample_gateway_logs\\Gateway1\n",
      "  Loaded CSV: QueryExecutionReport_20250919.csv (3 rows)\n",
      "Found 1 query start files in c:\\Users\\aveekr\\OneDrive - Microsoft\\Documents\\Demos\\my_code_samples\\azuresamples\\sample_gateway_logs\\Gateway1\n",
      "  Loaded CSV: QueryStartReport_20250919.csv (3 rows)\n",
      "\n",
      "Processing folder: Gateway2\n",
      "------------------------------\n",
      "Found 1 query execution files in c:\\Users\\aveekr\\OneDrive - Microsoft\\Documents\\Demos\\my_code_samples\\azuresamples\\sample_gateway_logs\\Gateway2\n",
      "  Loaded CSV: QueryExecutionReport_20250919.csv (3 rows)\n",
      "Found 1 query start files in c:\\Users\\aveekr\\OneDrive - Microsoft\\Documents\\Demos\\my_code_samples\\azuresamples\\sample_gateway_logs\\Gateway2\n",
      "  Loaded CSV: QueryStartReport_20250919.csv (3 rows)\n",
      "\n",
      "==================================================\n",
      "Data loading summary:\n",
      "Total execution report datasets: 2\n",
      "Total start report datasets: 2\n"
     ]
    }
   ],
   "source": [
    "# Main data loading script\n",
    "all_execution_data = []\n",
    "all_start_data = []\n",
    "\n",
    "print(\"Starting data loading process...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Process each folder in the log path\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(log_file_path, folder)\n",
    "    print(f\"\\nProcessing folder: {folder}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Load query execution reports from this folder\n",
    "    execution_data = load_query_execution_reports(folder_path)\n",
    "    all_execution_data.extend(execution_data)\n",
    "    \n",
    "    # Load query start reports from this folder\n",
    "    start_data = load_query_start_reports(folder_path)\n",
    "    all_start_data.extend(start_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Data loading summary:\")\n",
    "print(f\"Total execution report datasets: {len(all_execution_data)}\")\n",
    "print(f\"Total start report datasets: {len(all_start_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b78d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Query Execution Report:\n",
      "Total rows: 6\n",
      "Columns: ['RequestId', 'StartTime', 'EndTime', 'Duration', 'Status', 'source_file', 'folder']\n",
      "Data sources: 1 files from 2 folders\n",
      "\n",
      "First 5 rows of execution data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "RequestId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "StartTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EndTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Duration",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "folder",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e78d3cd0-c282-4ab5-ac3e-bf00be05cd92",
       "rows": [
        [
         "0",
         "req1",
         "2025-09-19 10:00:00",
         "2025-09-19 10:00:05",
         "5000",
         "Success",
         "QueryExecutionReport_20250919.csv",
         "Gateway1"
        ],
        [
         "1",
         "req2",
         "2025-09-19 10:01:00",
         "2025-09-19 10:01:03",
         "3000",
         "Success",
         "QueryExecutionReport_20250919.csv",
         "Gateway1"
        ],
        [
         "2",
         "req3",
         "2025-09-19 10:02:00",
         "2025-09-19 10:02:02",
         "2000",
         "Failed",
         "QueryExecutionReport_20250919.csv",
         "Gateway1"
        ],
        [
         "3",
         "req1",
         "2025-09-19 10:00:00",
         "2025-09-19 10:00:05",
         "5000",
         "Success",
         "QueryExecutionReport_20250919.csv",
         "Gateway2"
        ],
        [
         "4",
         "req2",
         "2025-09-19 10:01:00",
         "2025-09-19 10:01:03",
         "3000",
         "Success",
         "QueryExecutionReport_20250919.csv",
         "Gateway2"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RequestId</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Status</th>\n",
       "      <th>source_file</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>req1</td>\n",
       "      <td>2025-09-19 10:00:00</td>\n",
       "      <td>2025-09-19 10:00:05</td>\n",
       "      <td>5000</td>\n",
       "      <td>Success</td>\n",
       "      <td>QueryExecutionReport_20250919.csv</td>\n",
       "      <td>Gateway1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>req2</td>\n",
       "      <td>2025-09-19 10:01:00</td>\n",
       "      <td>2025-09-19 10:01:03</td>\n",
       "      <td>3000</td>\n",
       "      <td>Success</td>\n",
       "      <td>QueryExecutionReport_20250919.csv</td>\n",
       "      <td>Gateway1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>req3</td>\n",
       "      <td>2025-09-19 10:02:00</td>\n",
       "      <td>2025-09-19 10:02:02</td>\n",
       "      <td>2000</td>\n",
       "      <td>Failed</td>\n",
       "      <td>QueryExecutionReport_20250919.csv</td>\n",
       "      <td>Gateway1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>req1</td>\n",
       "      <td>2025-09-19 10:00:00</td>\n",
       "      <td>2025-09-19 10:00:05</td>\n",
       "      <td>5000</td>\n",
       "      <td>Success</td>\n",
       "      <td>QueryExecutionReport_20250919.csv</td>\n",
       "      <td>Gateway2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>req2</td>\n",
       "      <td>2025-09-19 10:01:00</td>\n",
       "      <td>2025-09-19 10:01:03</td>\n",
       "      <td>3000</td>\n",
       "      <td>Success</td>\n",
       "      <td>QueryExecutionReport_20250919.csv</td>\n",
       "      <td>Gateway2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RequestId            StartTime              EndTime  Duration   Status  \\\n",
       "0      req1  2025-09-19 10:00:00  2025-09-19 10:00:05      5000  Success   \n",
       "1      req2  2025-09-19 10:01:00  2025-09-19 10:01:03      3000  Success   \n",
       "2      req3  2025-09-19 10:02:00  2025-09-19 10:02:02      2000   Failed   \n",
       "3      req1  2025-09-19 10:00:00  2025-09-19 10:00:05      5000  Success   \n",
       "4      req2  2025-09-19 10:01:00  2025-09-19 10:01:03      3000  Success   \n",
       "\n",
       "                         source_file    folder  \n",
       "0  QueryExecutionReport_20250919.csv  Gateway1  \n",
       "1  QueryExecutionReport_20250919.csv  Gateway1  \n",
       "2  QueryExecutionReport_20250919.csv  Gateway1  \n",
       "3  QueryExecutionReport_20250919.csv  Gateway2  \n",
       "4  QueryExecutionReport_20250919.csv  Gateway2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Query Start Report:\n",
      "Total rows: 6\n",
      "Columns: ['RequestId', 'StartTime', 'QueryType', 'DataSource', 'source_file', 'folder']\n",
      "Data sources: 1 files from 2 folders\n",
      "\n",
      "First 5 rows of start data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "RequestId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "StartTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "QueryType",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DataSource",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "folder",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "8459f3cd-1ab1-4c5f-8533-abbdda3e77d1",
       "rows": [
        [
         "0",
         "req1",
         "2025-09-19 10:00:00",
         "DirectQuery",
         "SQL Server",
         "QueryStartReport_20250919.csv",
         "Gateway1"
        ],
        [
         "1",
         "req2",
         "2025-09-19 10:01:00",
         "Import",
         "SharePoint",
         "QueryStartReport_20250919.csv",
         "Gateway1"
        ],
        [
         "2",
         "req3",
         "2025-09-19 10:02:00",
         "DirectQuery",
         "SQL Server",
         "QueryStartReport_20250919.csv",
         "Gateway1"
        ],
        [
         "3",
         "req1",
         "2025-09-19 10:00:00",
         "DirectQuery",
         "SQL Server",
         "QueryStartReport_20250919.csv",
         "Gateway2"
        ],
        [
         "4",
         "req2",
         "2025-09-19 10:01:00",
         "Import",
         "SharePoint",
         "QueryStartReport_20250919.csv",
         "Gateway2"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RequestId</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>QueryType</th>\n",
       "      <th>DataSource</th>\n",
       "      <th>source_file</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>req1</td>\n",
       "      <td>2025-09-19 10:00:00</td>\n",
       "      <td>DirectQuery</td>\n",
       "      <td>SQL Server</td>\n",
       "      <td>QueryStartReport_20250919.csv</td>\n",
       "      <td>Gateway1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>req2</td>\n",
       "      <td>2025-09-19 10:01:00</td>\n",
       "      <td>Import</td>\n",
       "      <td>SharePoint</td>\n",
       "      <td>QueryStartReport_20250919.csv</td>\n",
       "      <td>Gateway1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>req3</td>\n",
       "      <td>2025-09-19 10:02:00</td>\n",
       "      <td>DirectQuery</td>\n",
       "      <td>SQL Server</td>\n",
       "      <td>QueryStartReport_20250919.csv</td>\n",
       "      <td>Gateway1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>req1</td>\n",
       "      <td>2025-09-19 10:00:00</td>\n",
       "      <td>DirectQuery</td>\n",
       "      <td>SQL Server</td>\n",
       "      <td>QueryStartReport_20250919.csv</td>\n",
       "      <td>Gateway2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>req2</td>\n",
       "      <td>2025-09-19 10:01:00</td>\n",
       "      <td>Import</td>\n",
       "      <td>SharePoint</td>\n",
       "      <td>QueryStartReport_20250919.csv</td>\n",
       "      <td>Gateway2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RequestId            StartTime    QueryType  DataSource  \\\n",
       "0      req1  2025-09-19 10:00:00  DirectQuery  SQL Server   \n",
       "1      req2  2025-09-19 10:01:00       Import  SharePoint   \n",
       "2      req3  2025-09-19 10:02:00  DirectQuery  SQL Server   \n",
       "3      req1  2025-09-19 10:00:00  DirectQuery  SQL Server   \n",
       "4      req2  2025-09-19 10:01:00       Import  SharePoint   \n",
       "\n",
       "                     source_file    folder  \n",
       "0  QueryStartReport_20250919.csv  Gateway1  \n",
       "1  QueryStartReport_20250919.csv  Gateway1  \n",
       "2  QueryStartReport_20250919.csv  Gateway1  \n",
       "3  QueryStartReport_20250919.csv  Gateway2  \n",
       "4  QueryStartReport_20250919.csv  Gateway2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine all execution data into single DataFrames\n",
    "if all_execution_data:\n",
    "    execution_df = pd.concat(all_execution_data, ignore_index=True)\n",
    "    print(f\"\\nCombined Query Execution Report:\")\n",
    "    print(f\"Total rows: {len(execution_df)}\")\n",
    "    print(f\"Columns: {list(execution_df.columns)}\")\n",
    "    print(f\"Data sources: {execution_df['source_file'].nunique()} files from {execution_df['folder'].nunique()} folders\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows of execution data:\")\n",
    "    display(execution_df.head())\n",
    "else:\n",
    "    execution_df = pd.DataFrame()\n",
    "    print(\"\\nNo query execution report data found\")\n",
    "\n",
    "if all_start_data:\n",
    "    start_df = pd.concat(all_start_data, ignore_index=True)\n",
    "    print(f\"\\nCombined Query Start Report:\")\n",
    "    print(f\"Total rows: {len(start_df)}\")\n",
    "    print(f\"Columns: {list(start_df.columns)}\")\n",
    "    print(f\"Data sources: {start_df['source_file'].nunique()} files from {start_df['folder'].nunique()} folders\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows of start data:\")\n",
    "    display(start_df.head())\n",
    "else:\n",
    "    start_df = pd.DataFrame()\n",
    "    print(\"\\nNo query start report data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6145fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save combined data to CSV files for further analysis\n",
    "save_to_csv = True  # Set to False if you don't want to save\n",
    "\n",
    "if save_to_csv:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if not execution_df.empty:\n",
    "        execution_filename = f\"combined_query_execution_report_{timestamp}.csv\"\n",
    "        execution_df.to_csv(execution_filename, index=False)\n",
    "        print(f\"Query execution data saved to: {execution_filename}\")\n",
    "    \n",
    "    if not start_df.empty:\n",
    "        start_filename = f\"combined_query_start_report_{timestamp}.csv\"\n",
    "        start_df.to_csv(start_filename, index=False)\n",
    "        print(f\"Query start data saved to: {start_filename}\")\n",
    "\n",
    "print(\"\\nData loading complete! You can now analyze the data using the variables:\")\n",
    "print(\"- execution_df: Contains all query execution report data\")\n",
    "print(\"- start_df: Contains all query start report data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
