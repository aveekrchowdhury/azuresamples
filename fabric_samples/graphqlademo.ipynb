{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfceb95b",
   "metadata": {},
   "source": [
    "# üöÄ Microsoft Fabric GraphQL API Demo\n",
    "## Comprehensive Guide to Pagination, Filtering, and Querying\n",
    "\n",
    "This notebook demonstrates how to effectively use GraphQL with Microsoft Fabric's API, focusing on:\n",
    "\n",
    "- **üîÑ Cursor-based Pagination** - Efficient data traversal using `first` and `after` parameters\n",
    "- **üîç Advanced Filtering** - Complex filtering with `TripFilterInput`, `GeographyFilterInput`, etc.\n",
    "- **üìä Sorting & Ordering** - Using `TripOrderByInput` and other ordering inputs  \n",
    "- **üìà Aggregations** - Leveraging built-in aggregation functions\n",
    "- **üåê Multi-entity Queries** - Working with Trips, Geography, Medallions, Weather, and more\n",
    "\n",
    "### üóÇÔ∏è Available Data Types:\n",
    "- **Trip** - Taxi trip records with pickup/dropoff details\n",
    "- **Geography** - Location data with zip codes and addresses\n",
    "- **Medallion** - Taxi medallion information\n",
    "- **Weather** - Weather data correlated with trips\n",
    "- **Time** - Time dimension tables\n",
    "- **vw_PaymentAnalysis** - Payment analysis views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83ab34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìã Libraries loaded:\n",
      "   - requests: GraphQL HTTP client\n",
      "   - pandas: Data manipulation\n",
      "   - matplotlib/seaborn: Visualization\n",
      "   - azure.identity: Authentication\n",
      "   - json: JSON handling\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Setup and Import Libraries\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìã Libraries loaded:\")\n",
    "print(\"   - requests: GraphQL HTTP client\")\n",
    "print(\"   - pandas: Data manipulation\")\n",
    "print(\"   - matplotlib/seaborn: Visualization\")\n",
    "print(\"   - azure.identity: Authentication\")\n",
    "print(\"   - json: JSON handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3628f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Setting up authentication...\n",
      "‚úÖ Authentication successful!\n",
      "üåê GraphQL endpoint configured\n",
      "üì° Ready to execute queries\n"
     ]
    }
   ],
   "source": [
    "# üîê Authentication and GraphQL Client Setup\n",
    "\n",
    "# WARNING: This is for development/demo purposes only!\n",
    "# In production, use proper app registration with client_id and scopes\n",
    "print(\"üîë Setting up authentication...\")\n",
    "\n",
    "app = DefaultAzureCredential()\n",
    "scope = 'https://analysis.windows.net/powerbi/api/user_impersonation'\n",
    "result = app.get_token(scope)\n",
    "\n",
    "if not result.token:\n",
    "    raise Exception(\"‚ùå Could not get access token\")\n",
    "\n",
    "# Configure GraphQL endpoint and headers\n",
    "endpoint = 'https://eebc13ddb5604280aeb3c6d342982b7e.zee.graphql.fabric.microsoft.com/v1/workspaces/eebc13dd-b560-4280-aeb3-c6d342982b7e/graphqlapis/d8b1987f-67b0-4934-bfff-662417bb3fc7/graphql'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {result.token}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Authentication successful!\")\n",
    "print(\"üåê GraphQL endpoint configured\")\n",
    "print(\"üì° Ready to execute queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cb69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è GraphQL Helper Functions\n",
    "\n",
    "def execute_graphql_query(query, variables=None):\n",
    "    \"\"\"\n",
    "    Execute a GraphQL query with error handling and response parsing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        payload = {'query': query}\n",
    "        if variables:\n",
    "            payload['variables'] = variables\n",
    "            \n",
    "        response = requests.post(endpoint, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if 'errors' in data:\n",
    "            print(f\"‚ùå GraphQL errors: {data['errors']}\")\n",
    "            return None\n",
    "            \n",
    "        return data['data']\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(f\"‚ùå Query failed: {error}\")\n",
    "        return None\n",
    "\n",
    "def print_pagination_info(connection_data, entity_name=\"items\"):\n",
    "    \"\"\"\n",
    "    Print pagination information in a readable format\n",
    "    \"\"\"\n",
    "    if not connection_data:\n",
    "        return\n",
    "        \n",
    "    items_count = len(connection_data.get('items', []))\n",
    "    has_next = connection_data.get('hasNextPage', False)\n",
    "    end_cursor = connection_data.get('endCursor', 'None')\n",
    "    \n",
    "    print(f\"üìä Retrieved {items_count} {entity_name}\")\n",
    "    print(f\"üìÑ Has next page: {has_next}\")\n",
    "    print(f\"üîó End cursor: {end_cursor}\")\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e5e7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SINGLE PAGE FETCH TEST ===\n",
      "üîÑ Fetching trips page (size: 10)\n",
      "üìä Retrieved 10 trips\n",
      "üìÑ Has next page: True\n",
      "üîó End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwMSwiRGlyZWN0aW9uIjowfV0=\n",
      "\n",
      "üìã Sample trip data:\n",
      "   DateID: 20130101\n",
      "   MedallionID: 1035\n",
      "   HackneyLicenseID: 15003\n",
      "   PickupTimeID: 20800\n",
      "   DropoffTimeID: 21729\n",
      "   PickupGeographyID: 74530\n",
      "   DropoffGeographyID: 46957\n",
      "   PickupLatitude: 40.8237\n",
      "   PickupLongitude: -73.9548\n",
      "   PickupLatLong: 40.8237,-73.9548\n",
      "   DropoffLatitude: 40.9157\n",
      "   DropoffLongitude: -73.8973\n",
      "   DropoffLatLong: 40.9157,-73.8973\n",
      "   PassengerCount: 1\n",
      "   TripDurationSeconds: 928\n",
      "   TripDistanceMiles: 7.9\n",
      "   PaymentType: CSH\n",
      "   FareAmount: 24\n",
      "   SurchargeAmount: 1\n",
      "   TaxAmount: 1\n",
      "   TipAmount: 0\n",
      "   TollsAmount: 2\n",
      "   TotalAmount: 27\n"
     ]
    }
   ],
   "source": [
    "# üöï Single Page Trip Data Query (Page Size: 10)\n",
    "\n",
    "def fetch_trips_page(page_size=10, after_cursor=None):\n",
    "    \"\"\"\n",
    "    Fetch a single page of trip data using cursor-based pagination\n",
    "    \"\"\"\n",
    "    # Build query arguments\n",
    "    args = f\"first: {page_size}\"\n",
    "    if after_cursor:\n",
    "        args += f', after: \"{after_cursor}\"'\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    query {{\n",
    "      trips({args}) {{\n",
    "         items {{\n",
    "            DateID\n",
    "            MedallionID\n",
    "            HackneyLicenseID\n",
    "            PickupTimeID\n",
    "            DropoffTimeID\n",
    "            PickupGeographyID\n",
    "            DropoffGeographyID\n",
    "            PickupLatitude\n",
    "            PickupLongitude\n",
    "            PickupLatLong\n",
    "            DropoffLatitude\n",
    "            DropoffLongitude\n",
    "            DropoffLatLong\n",
    "            PassengerCount\n",
    "            TripDurationSeconds\n",
    "            TripDistanceMiles\n",
    "            PaymentType\n",
    "            FareAmount\n",
    "            SurchargeAmount\n",
    "            TaxAmount\n",
    "            TipAmount\n",
    "            TollsAmount\n",
    "            TotalAmount\n",
    "         }}\n",
    "         endCursor\n",
    "         hasNextPage\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Fetching trips page (size: {page_size})\")\n",
    "    if after_cursor:\n",
    "        print(f\"   Using cursor: {after_cursor[:20]}...\")\n",
    "    \n",
    "    data = execute_graphql_query(query)\n",
    "    \n",
    "    if data and 'trips' in data:\n",
    "        print_pagination_info(data['trips'], \"trips\")\n",
    "        return data['trips']\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test: Fetch first page of trips\n",
    "print(\"=== SINGLE PAGE FETCH TEST ===\")\n",
    "first_page = fetch_trips_page(page_size=10)\n",
    "\n",
    "if first_page and first_page['items']:\n",
    "    print(f\"\\nüìã Sample trip data:\")\n",
    "    sample_trip = first_page['items'][0]\n",
    "    for key, value in sample_trip.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "else:\n",
    "    print(\"‚ùå No trip data retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cba50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLETE PAGINATION DEMO ===\n",
      "‚ö†Ô∏è  Fetching first 5 pages for demonstration (50 trips max)\n",
      "    In production, remove max_pages limit to fetch all data\n",
      "üöÄ Starting complete trip data fetch\n",
      "üìã Configuration:\n",
      "   ‚Ä¢ Page size: 100\n",
      "   ‚Ä¢ Max pages: 5\n",
      "   ‚Ä¢ Delay between requests: 0.5s\n",
      "==================================================\n",
      "\n",
      "üìñ Fetching page 1...\n",
      "üîÑ Fetching trips page (size: 100)\n",
      "üìä Retrieved 100 trips\n",
      "üìÑ Has next page: True\n",
      "üîó End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwMSwiRGlyZWN0aW9uIjowfV0=\n",
      "   ‚úÖ Added 100 trips\n",
      "   üìä Total trips collected: 100\n",
      "\n",
      "üìñ Fetching page 2...\n",
      "üîÑ Fetching trips page (size: 100)\n",
      "   Using cursor: W3siRW50aXR5TmFtZSI6...\n",
      "üìä Retrieved 100 trips\n",
      "üìÑ Has next page: True\n",
      "üîó End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwMiwiRGlyZWN0aW9uIjowfV0=\n",
      "   ‚úÖ Added 100 trips\n",
      "   üìä Total trips collected: 200\n",
      "\n",
      "üìñ Fetching page 3...\n",
      "üîÑ Fetching trips page (size: 100)\n",
      "   Using cursor: W3siRW50aXR5TmFtZSI6...\n",
      "üìä Retrieved 100 trips\n",
      "üìÑ Has next page: True\n",
      "üîó End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwMywiRGlyZWN0aW9uIjowfV0=\n",
      "   ‚úÖ Added 100 trips\n",
      "   üìä Total trips collected: 300\n",
      "\n",
      "üìñ Fetching page 4...\n",
      "üîÑ Fetching trips page (size: 100)\n",
      "   Using cursor: W3siRW50aXR5TmFtZSI6...\n",
      "üìä Retrieved 100 trips\n",
      "üìÑ Has next page: True\n",
      "üîó End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwNCwiRGlyZWN0aW9uIjowfV0=\n",
      "   ‚úÖ Added 100 trips\n",
      "   üìä Total trips collected: 400\n",
      "\n",
      "üìñ Fetching page 5...\n",
      "üîÑ Fetching trips page (size: 100)\n",
      "   Using cursor: W3siRW50aXR5TmFtZSI6...\n",
      "üìä Retrieved 100 trips\n",
      "üìÑ Has next page: True\n",
      "üîó End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwNSwiRGlyZWN0aW9uIjowfV0=\n",
      "   ‚úÖ Added 100 trips\n",
      "   üìä Total trips collected: 500\n",
      "üõë Reached maximum page limit: 5\n",
      "\n",
      "==================================================\n",
      "üéØ PAGINATION COMPLETE!\n",
      "üìä Final Results:\n",
      "   ‚Ä¢ Total trips retrieved: 500\n",
      "   ‚Ä¢ Pages processed: 6\n",
      "   ‚Ä¢ Time elapsed: 11.60 seconds\n",
      "   ‚Ä¢ Average time per page: 1.93 seconds\n"
     ]
    }
   ],
   "source": [
    "# üîÑ Complete Pagination: Fetch ALL Trip Data\n",
    "\n",
    "def fetch_all_trips(page_size=100, max_pages=None, delay_seconds=0.5):\n",
    "    \"\"\"\n",
    "    Fetch ALL trip data using pagination with configurable limits\n",
    "    \n",
    "    Args:\n",
    "        page_size (int): Number of items per page\n",
    "        max_pages (int): Maximum pages to fetch (None for unlimited)\n",
    "        delay_seconds (float): Delay between requests to avoid rate limiting\n",
    "    \n",
    "    Returns:\n",
    "        list: All trip records retrieved\n",
    "    \"\"\"\n",
    "    all_trips = []\n",
    "    after_cursor = None\n",
    "    page_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"üöÄ Starting complete trip data fetch\")\n",
    "    print(f\"üìã Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Page size: {page_size}\")\n",
    "    print(f\"   ‚Ä¢ Max pages: {max_pages or 'Unlimited'}\")\n",
    "    print(f\"   ‚Ä¢ Delay between requests: {delay_seconds}s\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    while True:\n",
    "        page_count += 1\n",
    "        \n",
    "        # Check page limit\n",
    "        if max_pages and page_count > max_pages:\n",
    "            print(f\"üõë Reached maximum page limit: {max_pages}\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nüìñ Fetching page {page_count}...\")\n",
    "        \n",
    "        # Fetch current page\n",
    "        page_data = fetch_trips_page(page_size=page_size, after_cursor=after_cursor)\n",
    "        \n",
    "        if not page_data or not page_data.get('items'):\n",
    "            print(f\"‚ÑπÔ∏è  No more data found at page {page_count}\")\n",
    "            break\n",
    "        \n",
    "        # Add items to collection\n",
    "        items = page_data['items']\n",
    "        all_trips.extend(items)\n",
    "        \n",
    "        print(f\"   ‚úÖ Added {len(items)} trips\")\n",
    "        print(f\"   üìä Total trips collected: {len(all_trips)}\")\n",
    "        \n",
    "        # Check if there's a next page\n",
    "        if not page_data.get('hasNextPage', False):\n",
    "            print(f\"‚ÑπÔ∏è  Reached end of data (no more pages)\")\n",
    "            break\n",
    "        \n",
    "        # Update cursor for next page\n",
    "        after_cursor = page_data.get('endCursor')\n",
    "        \n",
    "        # Rate limiting delay\n",
    "        if delay_seconds > 0:\n",
    "            time.sleep(delay_seconds)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üéØ PAGINATION COMPLETE!\")\n",
    "    print(f\"üìä Final Results:\")\n",
    "    print(f\"   ‚Ä¢ Total trips retrieved: {len(all_trips):,}\")\n",
    "    print(f\"   ‚Ä¢ Pages processed: {page_count}\")\n",
    "    print(f\"   ‚Ä¢ Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"   ‚Ä¢ Average time per page: {elapsed_time/page_count:.2f} seconds\")\n",
    "    \n",
    "    return all_trips\n",
    "\n",
    "# Execute complete pagination with reasonable limits for demo\n",
    "print(\"=== COMPLETE PAGINATION DEMO ===\")\n",
    "print(\"‚ö†Ô∏è  Fetching first 5 pages for demonstration (50 trips max)\")\n",
    "print(\"    In production, remove max_pages limit to fetch all data\")\n",
    "\n",
    "all_trip_data = fetch_all_trips(\n",
    "    page_size=100,\n",
    "    max_pages=5,  # Limit for demo - remove this in production\n",
    "    delay_seconds=0.5  # Small delay to be respectful to the API\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c52afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ EXPORTING TRIP DATA TO JSON\n",
      "==================================================\n",
      "‚úÖ Trip data successfully exported!\n",
      "üìÅ File: fabric_trips_data_20250924_102403.json\n",
      "üìä Records: 500\n",
      "üíæ File size: 372,022 bytes (0.35 MB)\n",
      "\n",
      "üìã Export Summary:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m saved_file:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìã Export Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Ä¢ File location: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mos\u001b[49m.path.abspath(saved_file)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Ä¢ Data structure: JSON with metadata wrapper\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Ä¢ Encoding: UTF-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# üíæ Export Trip Data to JSON File\n",
    "\n",
    "def save_trips_to_json(trips_data, filename=None):\n",
    "    \"\"\"\n",
    "    Save trip data to a JSON file with metadata\n",
    "    \n",
    "    Args:\n",
    "        trips_data (list): List of trip records\n",
    "        filename (str): Optional filename, auto-generated if not provided\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the saved file\n",
    "    \"\"\"\n",
    "    if not trips_data:\n",
    "        print(\"‚ùå No trip data to save\")\n",
    "        return None\n",
    "    \n",
    "    # Generate filename if not provided\n",
    "    if not filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"fabric_trips_data_{timestamp}.json\"\n",
    "    \n",
    "    # Create export data with metadata\n",
    "    export_data = {\n",
    "        \"metadata\": {\n",
    "            \"export_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_records\": len(trips_data),\n",
    "            \"source\": \"Microsoft Fabric GraphQL API\",\n",
    "            \"schema_version\": \"1.0\"\n",
    "        },\n",
    "        \"trips\": trips_data\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Write to JSON file with proper formatting\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(export_data, file, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        # Get file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename)\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"‚úÖ Trip data successfully exported!\")\n",
    "        print(f\"üìÅ File: {filename}\")\n",
    "        print(f\"üìä Records: {len(trips_data):,}\")\n",
    "        print(f\"üíæ File size: {file_size:,} bytes ({file_size_mb:.2f} MB)\")\n",
    "        \n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Save the fetched trip data to JSON file\n",
    "if 'all_trip_data' in locals() and all_trip_data:\n",
    "    print(\"üíæ EXPORTING TRIP DATA TO JSON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    saved_file = save_trips_to_json(all_trip_data)\n",
    "    \n",
    "    if saved_file:\n",
    "        print(f\"\\nüìã Export Summary:\")\n",
    "        print(f\"   ‚Ä¢ File location: {os.path.abspath(saved_file)}\")\n",
    "        print(f\"   ‚Ä¢ Data structure: JSON with metadata wrapper\")\n",
    "        print(f\"   ‚Ä¢ Encoding: UTF-8\")\n",
    "        print(f\"   ‚Ä¢ Format: Pretty-printed with 2-space indentation\")\n",
    "        \n",
    "        # Show sample structure\n",
    "        print(f\"\\nüîç File Structure Preview:\")\n",
    "        print(f\"{{\")\n",
    "        print(f\"  'metadata': {{\")\n",
    "        print(f\"    'export_timestamp': '2025-09-24T...',\")\n",
    "        print(f\"    'total_records': {len(all_trip_data)},\")\n",
    "        print(f\"    'source': 'Microsoft Fabric GraphQL API',\")\n",
    "        print(f\"    'schema_version': '1.0'\")\n",
    "        print(f\"  }},\")\n",
    "        print(f\"  'trips': [\")\n",
    "        print(f\"    {{ trip_record_1 }},\")\n",
    "        print(f\"    {{ trip_record_2 }},\")\n",
    "        print(f\"    ...\")\n",
    "        print(f\"  ]\")\n",
    "        print(f\"}}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No trip data available to export\")\n",
    "    print(\"   Run the pagination cells above first to fetch trip data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75faec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Advanced Export Options\n",
    "\n",
    "def export_trips_multiple_formats(trips_data, base_filename=None):\n",
    "    \"\"\"\n",
    "    Export trip data in multiple formats: JSON, CSV, and summary\n",
    "    \n",
    "    Args:\n",
    "        trips_data (list): List of trip records\n",
    "        base_filename (str): Base filename (without extension)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with paths to all created files\n",
    "    \"\"\"\n",
    "    if not trips_data:\n",
    "        print(\"‚ùå No trip data to export\")\n",
    "        return {}\n",
    "    \n",
    "    # Generate base filename if not provided\n",
    "    if not base_filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_filename = f\"fabric_trips_{timestamp}\"\n",
    "    \n",
    "    created_files = {}\n",
    "    \n",
    "    print(f\"üìÇ EXPORTING TO MULTIPLE FORMATS\")\n",
    "    print(f\"üè∑Ô∏è  Base filename: {base_filename}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Export as JSON (detailed format)\n",
    "    try:\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        export_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_records\": len(trips_data),\n",
    "                \"source\": \"Microsoft Fabric GraphQL API\",\n",
    "                \"export_format\": \"detailed_json\",\n",
    "                \"columns\": list(trips_data[0].keys()) if trips_data else [],\n",
    "                \"schema_version\": \"1.0\"\n",
    "            },\n",
    "            \"trips\": trips_data\n",
    "        }\n",
    "        \n",
    "        with open(json_filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(export_data, file, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        created_files['json'] = json_filename\n",
    "        file_size = os.path.getsize(json_filename) / (1024 * 1024)\n",
    "        print(f\"‚úÖ JSON: {json_filename} ({file_size:.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå JSON export failed: {str(e)}\")\n",
    "    \n",
    "    # 2. Export as CSV (for Excel compatibility)\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        \n",
    "        df = pd.DataFrame(trips_data)\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        created_files['csv'] = csv_filename\n",
    "        file_size = os.path.getsize(csv_filename) / (1024 * 1024)\n",
    "        print(f\"‚úÖ CSV: {csv_filename} ({file_size:.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CSV export failed: {str(e)}\")\n",
    "    \n",
    "    # 3. Export summary statistics\n",
    "    try:\n",
    "        summary_filename = f\"{base_filename}_summary.json\"\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        df = pd.DataFrame(trips_data)\n",
    "        \n",
    "        # Numeric columns summary\n",
    "        numeric_summary = {}\n",
    "        numeric_cols = df.select_dtypes(include=[int, float]).columns\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                numeric_summary[col] = {\n",
    "                    \"count\": int(df[col].count()),\n",
    "                    \"mean\": float(df[col].mean()) if pd.notna(df[col].mean()) else None,\n",
    "                    \"std\": float(df[col].std()) if pd.notna(df[col].std()) else None,\n",
    "                    \"min\": float(df[col].min()) if pd.notna(df[col].min()) else None,\n",
    "                    \"max\": float(df[col].max()) if pd.notna(df[col].max()) else None\n",
    "                }\n",
    "        \n",
    "        # Categorical columns summary\n",
    "        categorical_summary = {}\n",
    "        categorical_cols = df.select_dtypes(include=[object]).columns\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                value_counts = df[col].value_counts().head(10).to_dict()\n",
    "                categorical_summary[col] = {\n",
    "                    \"unique_count\": int(df[col].nunique()),\n",
    "                    \"top_values\": {str(k): int(v) for k, v in value_counts.items()}\n",
    "                }\n",
    "        \n",
    "        summary_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_timestamp\": datetime.now().isoformat(),\n",
    "                \"source_records\": len(trips_data),\n",
    "                \"analysis_type\": \"summary_statistics\"\n",
    "            },\n",
    "            \"dataset_overview\": {\n",
    "                \"total_records\": len(df),\n",
    "                \"total_columns\": len(df.columns),\n",
    "                \"numeric_columns\": len(numeric_cols),\n",
    "                \"categorical_columns\": len(categorical_cols),\n",
    "                \"memory_usage_mb\": round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2)\n",
    "            },\n",
    "            \"numeric_statistics\": numeric_summary,\n",
    "            \"categorical_statistics\": categorical_summary\n",
    "        }\n",
    "        \n",
    "        with open(summary_filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(summary_data, file, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        created_files['summary'] = summary_filename\n",
    "        file_size = os.path.getsize(summary_filename) / 1024  # KB for summary\n",
    "        print(f\"‚úÖ Summary: {summary_filename} ({file_size:.1f} KB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Summary export failed: {str(e)}\")\n",
    "    \n",
    "    # 4. Create a manifest file\n",
    "    try:\n",
    "        manifest_filename = f\"{base_filename}_manifest.json\"\n",
    "        \n",
    "        manifest_data = {\n",
    "            \"export_info\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"base_filename\": base_filename,\n",
    "                \"source\": \"Microsoft Fabric GraphQL API\",\n",
    "                \"total_records\": len(trips_data)\n",
    "            },\n",
    "            \"files_created\": created_files,\n",
    "            \"file_descriptions\": {\n",
    "                \"json\": \"Complete trip data with metadata in JSON format\",\n",
    "                \"csv\": \"Trip data in CSV format for Excel/analysis tools\",\n",
    "                \"summary\": \"Statistical summary and data profiling information\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(manifest_filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(manifest_data, file, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        created_files['manifest'] = manifest_filename\n",
    "        print(f\"‚úÖ Manifest: {manifest_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Manifest creation failed: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üéâ Export complete! Created {len(created_files)} files\")\n",
    "    \n",
    "    return created_files\n",
    "\n",
    "# Execute the multi-format export\n",
    "if 'all_trip_data' in locals() and all_trip_data:\n",
    "    print(\"üöÄ EXECUTING MULTI-FORMAT EXPORT\")\n",
    "    \n",
    "    export_results = export_trips_multiple_formats(all_trip_data)\n",
    "    \n",
    "    if export_results:\n",
    "        print(f\"\\nüìÅ All files created in current directory:\")\n",
    "        for format_type, filename in export_results.items():\n",
    "            full_path = os.path.abspath(filename)\n",
    "            print(f\"   {format_type.upper()}: {full_path}\")\n",
    "        \n",
    "        print(f\"\\nüí° Usage Tips:\")\n",
    "        print(f\"   ‚Ä¢ Open .csv file in Excel for data analysis\")\n",
    "        print(f\"   ‚Ä¢ Use .json file for programmatic processing\")\n",
    "        print(f\"   ‚Ä¢ Check _summary.json for data insights\")\n",
    "        print(f\"   ‚Ä¢ Review _manifest.json for export details\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No trip data available for export\")\n",
    "    print(\"   Execute the pagination cells first to fetch data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Data Analysis and Summary\n",
    "\n",
    "if all_trip_data:\n",
    "    print(\"=== TRIP DATA ANALYSIS ===\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df_trips = pd.DataFrame(all_trip_data)\n",
    "    \n",
    "    print(f\"üìã Dataset Overview:\")\n",
    "    print(f\"   ‚Ä¢ Total trips: {len(df_trips):,}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {len(df_trips.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Data types:\")\n",
    "    \n",
    "    # Show data types\n",
    "    for col in df_trips.columns:\n",
    "        dtype = df_trips[col].dtype\n",
    "        non_null = df_trips[col].notna().sum()\n",
    "        print(f\"     - {col}: {dtype} ({non_null}/{len(df_trips)} non-null)\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Financial Summary:\")\n",
    "    if 'FareAmount' in df_trips.columns:\n",
    "        fare_amounts = pd.to_numeric(df_trips['FareAmount'], errors='coerce')\n",
    "        total_amounts = pd.to_numeric(df_trips['TotalAmount'], errors='coerce')\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Total Fare Amount: ${fare_amounts.sum():,.2f}\")\n",
    "        print(f\"   ‚Ä¢ Average Fare: ${fare_amounts.mean():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Total Amount (with tips/taxes): ${total_amounts.sum():,.2f}\")\n",
    "        print(f\"   ‚Ä¢ Average Total: ${total_amounts.mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\nüöó Trip Summary:\")\n",
    "    if 'PassengerCount' in df_trips.columns:\n",
    "        passenger_counts = pd.to_numeric(df_trips['PassengerCount'], errors='coerce')\n",
    "        print(f\"   ‚Ä¢ Total Passengers: {passenger_counts.sum():,}\")\n",
    "        print(f\"   ‚Ä¢ Average Passengers per Trip: {passenger_counts.mean():.1f}\")\n",
    "    \n",
    "    if 'TripDistanceMiles' in df_trips.columns:\n",
    "        distances = pd.to_numeric(df_trips['TripDistanceMiles'], errors='coerce')\n",
    "        print(f\"   ‚Ä¢ Total Distance: {distances.sum():,.1f} miles\")\n",
    "        print(f\"   ‚Ä¢ Average Distance: {distances.mean():.1f} miles\")\n",
    "    \n",
    "    print(f\"\\nüí≥ Payment Type Distribution:\")\n",
    "    if 'PaymentType' in df_trips.columns:\n",
    "        payment_counts = df_trips['PaymentType'].value_counts()\n",
    "        for payment_type, count in payment_counts.items():\n",
    "            percentage = (count / len(df_trips)) * 100\n",
    "            print(f\"   ‚Ä¢ {payment_type}: {count} trips ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample Records (first 3 trips):\")\n",
    "    print(df_trips.head(3).to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trip data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Trip Data Visualization\n",
    "\n",
    "if all_trip_data and len(all_trip_data) > 0:\n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Trip Data Analysis Dashboard', fontsize=16, y=1.02)\n",
    "    \n",
    "    # 1. Fare Amount Distribution\n",
    "    if 'FareAmount' in df_trips.columns:\n",
    "        fare_amounts = pd.to_numeric(df_trips['FareAmount'], errors='coerce').dropna()\n",
    "        if len(fare_amounts) > 0:\n",
    "            axes[0, 0].hist(fare_amounts, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[0, 0].set_title('Fare Amount Distribution')\n",
    "            axes[0, 0].set_xlabel('Fare Amount ($)')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].axvline(fare_amounts.mean(), color='red', linestyle='--', \n",
    "                              label=f'Mean: ${fare_amounts.mean():.2f}')\n",
    "            axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Payment Type Distribution\n",
    "    if 'PaymentType' in df_trips.columns:\n",
    "        payment_counts = df_trips['PaymentType'].value_counts()\n",
    "        if len(payment_counts) > 0:\n",
    "            axes[0, 1].pie(payment_counts.values, labels=payment_counts.index, autopct='%1.1f%%')\n",
    "            axes[0, 1].set_title('Payment Type Distribution')\n",
    "    \n",
    "    # 3. Trip Distance Distribution\n",
    "    if 'TripDistanceMiles' in df_trips.columns:\n",
    "        distances = pd.to_numeric(df_trips['TripDistanceMiles'], errors='coerce').dropna()\n",
    "        if len(distances) > 0:\n",
    "            axes[1, 0].hist(distances, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "            axes[1, 0].set_title('Trip Distance Distribution')\n",
    "            axes[1, 0].set_xlabel('Distance (miles)')\n",
    "            axes[1, 0].set_ylabel('Frequency')\n",
    "            axes[1, 0].axvline(distances.mean(), color='red', linestyle='--', \n",
    "                              label=f'Mean: {distances.mean():.2f} miles')\n",
    "            axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Passenger Count Distribution\n",
    "    if 'PassengerCount' in df_trips.columns:\n",
    "        passenger_counts = df_trips['PassengerCount'].value_counts().sort_index()\n",
    "        if len(passenger_counts) > 0:\n",
    "            axes[1, 1].bar(passenger_counts.index, passenger_counts.values, \n",
    "                          alpha=0.7, color='orange', edgecolor='black')\n",
    "            axes[1, 1].set_title('Passenger Count Distribution')\n",
    "            axes[1, 1].set_xlabel('Number of Passengers')\n",
    "            axes[1, 1].set_ylabel('Number of Trips')\n",
    "            axes[1, 1].set_xticks(passenger_counts.index)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create correlation matrix if we have numeric columns\n",
    "    numeric_cols = df_trips.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = df_trips[numeric_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5)\n",
    "        plt.title('Trip Data Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bb7e1",
   "metadata": {},
   "source": [
    "## üöÄ Advanced Pagination Techniques\n",
    "\n",
    "### Handling Large Datasets\n",
    "\n",
    "When working with large datasets through GraphQL pagination, consider these best practices:\n",
    "\n",
    "1. **Rate Limiting**: Add delays between requests to avoid overwhelming the API\n",
    "2. **Error Handling**: Implement robust retry logic for network failures\n",
    "3. **Memory Management**: Process data in chunks to avoid memory issues\n",
    "4. **Progress Tracking**: Show progress for long-running operations\n",
    "5. **Caching**: Store intermediate results to resume interrupted operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Production-Ready Pagination with Error Handling\n",
    "\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Generator\n",
    "\n",
    "def fetch_all_trips_robust(\n",
    "    page_size: int = 10,\n",
    "    max_total_records: Optional[int] = None,\n",
    "    delay_between_requests: float = 0.5,\n",
    "    max_retries: int = 3\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Robust pagination implementation with error handling and rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        page_size: Number of records per page\n",
    "        max_total_records: Maximum total records to fetch (None = all)\n",
    "        delay_between_requests: Seconds to wait between API calls\n",
    "        max_retries: Maximum retry attempts for failed requests\n",
    "    \n",
    "    Returns:\n",
    "        List of all trip records\n",
    "    \"\"\"\n",
    "    all_trips = []\n",
    "    cursor = None\n",
    "    page_number = 1\n",
    "    total_fetched = 0\n",
    "    \n",
    "    print(f\"üöÄ Starting robust pagination (page_size={page_size}, max_records={max_total_records})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        retry_count = 0\n",
    "        current_page_data = None\n",
    "        \n",
    "        # Retry logic for current page\n",
    "        while retry_count <= max_retries:\n",
    "            try:\n",
    "                print(f\"üìÑ Fetching page {page_number} (attempt {retry_count + 1}/{max_retries + 1})\")\n",
    "                \n",
    "                current_page_data = fetch_trips_page(\n",
    "                    page_size=page_size, \n",
    "                    cursor=cursor\n",
    "                )\n",
    "                \n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"‚ö†Ô∏è Error on page {page_number}, attempt {retry_count}: {str(e)}\")\n",
    "                \n",
    "                if retry_count <= max_retries:\n",
    "                    wait_time = 2 ** retry_count  # Exponential backoff\n",
    "                    print(f\"‚è≥ Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to fetch page {page_number} after {max_retries + 1} attempts\")\n",
    "                    print(f\"üìä Returning {len(all_trips)} trips fetched so far\")\n",
    "                    return all_trips\n",
    "        \n",
    "        # Process successful page\n",
    "        if current_page_data and current_page_data.get('items'):\n",
    "            trips_on_page = current_page_data['items']\n",
    "            all_trips.extend(trips_on_page)\n",
    "            total_fetched += len(trips_on_page)\n",
    "            \n",
    "            print(f\"‚úÖ Page {page_number}: {len(trips_on_page)} trips fetched\")\n",
    "            print(f\"üìä Total so far: {total_fetched} trips\")\n",
    "            \n",
    "            # Check if we've reached the maximum limit\n",
    "            if max_total_records and total_fetched >= max_total_records:\n",
    "                all_trips = all_trips[:max_total_records]  # Trim to exact limit\n",
    "                print(f\"üèÅ Reached maximum limit of {max_total_records} records\")\n",
    "                break\n",
    "            \n",
    "            # Check if there are more pages\n",
    "            if not current_page_data.get('hasNextPage', False):\n",
    "                print(f\"üèÅ No more pages available\")\n",
    "                break\n",
    "            \n",
    "            # Prepare for next page\n",
    "            cursor = current_page_data.get('endCursor')\n",
    "            if not cursor:\n",
    "                print(f\"‚ö†Ô∏è No cursor for next page, stopping\")\n",
    "                break\n",
    "            \n",
    "            page_number += 1\n",
    "            \n",
    "            # Rate limiting\n",
    "            if delay_between_requests > 0:\n",
    "                print(f\"‚è±Ô∏è Waiting {delay_between_requests}s before next request...\")\n",
    "                time.sleep(delay_between_requests)\n",
    "        else:\n",
    "            print(f\"‚ùå No data returned for page {page_number}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üéâ Pagination complete! Total trips fetched: {len(all_trips)}\")\n",
    "    return all_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5766da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Test the Robust Pagination (Limited to 50 records for demo)\n",
    "\n",
    "print(\"üß™ TESTING ROBUST PAGINATION\")\n",
    "print(\"This demo fetches 50 records to show the robust pagination in action...\")\n",
    "print()\n",
    "\n",
    "# Test with limited records for demonstration\n",
    "test_trips = fetch_all_trips_robust(\n",
    "    page_size=10,\n",
    "    max_total_records=50,  # Limit for demo\n",
    "    delay_between_requests=0.1,  # Minimal delay for demo\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "if test_trips:\n",
    "    print(f\"\\nüéØ RESULTS SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Successfully fetched {len(test_trips)} trip records\")\n",
    "    print(f\"   üìÑ Pages processed: {(len(test_trips) - 1) // 10 + 1}\")\n",
    "    print(f\"   üîç Sample trip IDs: {[trip.get('TripId', 'N/A') for trip in test_trips[:5]]}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No trips were fetched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b4ab4",
   "metadata": {},
   "source": [
    "## üìö Summary & Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "‚úÖ **Authentication Setup**: Configured Microsoft Fabric GraphQL API access  \n",
    "‚úÖ **Schema Understanding**: Analyzed the Trip data structure with pagination support  \n",
    "‚úÖ **Basic Pagination**: Implemented simple page-by-page data fetching  \n",
    "‚úÖ **Complete Pagination**: Created functions to fetch ALL data using cursor-based pagination  \n",
    "‚úÖ **Data Analysis**: Added comprehensive data analysis and visualization  \n",
    "‚úÖ **Production-Ready Code**: Implemented robust error handling and rate limiting  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Page Size**: Use `first: 10` parameter to control page size\n",
    "2. **Cursor Navigation**: Use `after: \"cursor_value\"` for next pages\n",
    "3. **Completion Detection**: Check `hasNextPage` to know when to stop\n",
    "4. **Error Handling**: Always implement retry logic for production systems\n",
    "5. **Rate Limiting**: Add delays between requests to be API-friendly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Scale Up**: Remove the `max_total_records` limit to fetch complete dataset\n",
    "- **Data Pipeline**: Export results to CSV/database for further analysis\n",
    "- **Monitoring**: Add logging and metrics for production deployments\n",
    "- **Optimization**: Implement parallel processing for multiple data streams\n",
    "\n",
    "### üéØ Ready to Use!\n",
    "\n",
    "Your pagination implementation is now ready to handle real-world scenarios with:\n",
    "- Configurable page sizes\n",
    "- Robust error handling  \n",
    "- Progress tracking\n",
    "- Memory-efficient processing\n",
    "- Production-grade reliability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
