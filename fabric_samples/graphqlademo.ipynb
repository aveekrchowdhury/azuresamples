{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfceb95b",
   "metadata": {},
   "source": [
    "# 🚀 Microsoft Fabric GraphQL API Demo\n",
    "## Comprehensive Guide to Pagination, Filtering, and Querying\n",
    "\n",
    "This notebook demonstrates how to effectively use GraphQL with Microsoft Fabric's API, focusing on:\n",
    "\n",
    "- **🔄 Cursor-based Pagination** - Efficient data traversal using `first` and `after` parameters\n",
    "- **🔍 Advanced Filtering** - Complex filtering with `TripFilterInput`, `GeographyFilterInput`, etc.\n",
    "- **📊 Sorting & Ordering** - Using `TripOrderByInput` and other ordering inputs  \n",
    "- **📈 Aggregations** - Leveraging built-in aggregation functions\n",
    "- **🌐 Multi-entity Queries** - Working with Trips, Geography, Medallions, Weather, and more\n",
    "\n",
    "### 🗂️ Available Data Types:\n",
    "- **Trip** - Taxi trip records with pickup/dropoff details\n",
    "- **Geography** - Location data with zip codes and addresses\n",
    "- **Medallion** - Taxi medallion information\n",
    "- **Weather** - Weather data correlated with trips\n",
    "- **Time** - Time dimension tables\n",
    "- **vw_PaymentAnalysis** - Payment analysis views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83ab34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "📋 Libraries loaded:\n",
      "   - requests: GraphQL HTTP client\n",
      "   - pandas: Data manipulation\n",
      "   - matplotlib/seaborn: Visualization\n",
      "   - azure.identity: Authentication\n",
      "   - json: JSON handling\n"
     ]
    }
   ],
   "source": [
    "# 📦 Setup and Import Libraries\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"📋 Libraries loaded:\")\n",
    "print(\"   - requests: GraphQL HTTP client\")\n",
    "print(\"   - pandas: Data manipulation\")\n",
    "print(\"   - matplotlib/seaborn: Visualization\")\n",
    "print(\"   - azure.identity: Authentication\")\n",
    "print(\"   - json: JSON handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3628f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Setting up authentication...\n",
      "✅ Authentication successful!\n",
      "🌐 GraphQL endpoint configured\n",
      "📡 Ready to execute queries\n"
     ]
    }
   ],
   "source": [
    "# 🔐 Authentication and GraphQL Client Setup\n",
    "\n",
    "# WARNING: This is for development/demo purposes only!\n",
    "# In production, use proper app registration with client_id and scopes\n",
    "print(\"🔑 Setting up authentication...\")\n",
    "\n",
    "app = DefaultAzureCredential()\n",
    "scope = 'https://analysis.windows.net/powerbi/api/user_impersonation'\n",
    "result = app.get_token(scope)\n",
    "\n",
    "if not result.token:\n",
    "    raise Exception(\"❌ Could not get access token\")\n",
    "\n",
    "# Configure GraphQL endpoint and headers\n",
    "endpoint = 'https://eebc13ddb5604280aeb3c6d342982b7e.zee.graphql.fabric.microsoft.com/v1/workspaces/eebc13dd-b560-4280-aeb3-c6d342982b7e/graphqlapis/d8b1987f-67b0-4934-bfff-662417bb3fc7/graphql'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {result.token}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "print(\"✅ Authentication successful!\")\n",
    "print(\"🌐 GraphQL endpoint configured\")\n",
    "print(\"📡 Ready to execute queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cb69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ GraphQL Helper Functions\n",
    "\n",
    "def execute_graphql_query(query, variables=None):\n",
    "    \"\"\"\n",
    "    Execute a GraphQL query with error handling and response parsing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        payload = {'query': query}\n",
    "        if variables:\n",
    "            payload['variables'] = variables\n",
    "            \n",
    "        response = requests.post(endpoint, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if 'errors' in data:\n",
    "            print(f\"❌ GraphQL errors: {data['errors']}\")\n",
    "            return None\n",
    "            \n",
    "        return data['data']\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(f\"❌ Query failed: {error}\")\n",
    "        return None\n",
    "\n",
    "def print_pagination_info(connection_data, entity_name=\"items\"):\n",
    "    \"\"\"\n",
    "    Print pagination information in a readable format\n",
    "    \"\"\"\n",
    "    if not connection_data:\n",
    "        return\n",
    "        \n",
    "    items_count = len(connection_data.get('items', []))\n",
    "    has_next = connection_data.get('hasNextPage', False)\n",
    "    end_cursor = connection_data.get('endCursor', 'None')\n",
    "    \n",
    "    print(f\"📊 Retrieved {items_count} {entity_name}\")\n",
    "    print(f\"📄 Has next page: {has_next}\")\n",
    "    print(f\"🔗 End cursor: {end_cursor}\")\n",
    "\n",
    "print(\"✅ Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e5e7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SINGLE PAGE FETCH TEST ===\n",
      "🔄 Fetching trips page (size: 10)\n",
      "📊 Retrieved 10 trips\n",
      "📄 Has next page: True\n",
      "🔗 End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwMSwiRGlyZWN0aW9uIjowfV0=\n",
      "\n",
      "📋 Sample trip data:\n",
      "   DateID: 20130101\n",
      "   MedallionID: 1035\n",
      "   HackneyLicenseID: 15003\n",
      "   PickupTimeID: 20800\n",
      "   DropoffTimeID: 21729\n",
      "   PickupGeographyID: 74530\n",
      "   DropoffGeographyID: 46957\n",
      "   PickupLatitude: 40.8237\n",
      "   PickupLongitude: -73.9548\n",
      "   PickupLatLong: 40.8237,-73.9548\n",
      "   DropoffLatitude: 40.9157\n",
      "   DropoffLongitude: -73.8973\n",
      "   DropoffLatLong: 40.9157,-73.8973\n",
      "   PassengerCount: 1\n",
      "   TripDurationSeconds: 928\n",
      "   TripDistanceMiles: 7.9\n",
      "   PaymentType: CSH\n",
      "   FareAmount: 24\n",
      "   SurchargeAmount: 1\n",
      "   TaxAmount: 1\n",
      "   TipAmount: 0\n",
      "   TollsAmount: 2\n",
      "   TotalAmount: 27\n"
     ]
    }
   ],
   "source": [
    "# 🚕 Single Page Trip Data Query (Page Size: 10)\n",
    "\n",
    "def fetch_trips_page(page_size=10, after_cursor=None):\n",
    "    \"\"\"\n",
    "    Fetch a single page of trip data using cursor-based pagination\n",
    "    \"\"\"\n",
    "    # Build query arguments\n",
    "    args = f\"first: {page_size}\"\n",
    "    if after_cursor:\n",
    "        args += f', after: \"{after_cursor}\"'\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    query {{\n",
    "      trips({args}) {{\n",
    "         items {{\n",
    "            DateID\n",
    "            MedallionID\n",
    "            HackneyLicenseID\n",
    "            PickupTimeID\n",
    "            DropoffTimeID\n",
    "            PickupGeographyID\n",
    "            DropoffGeographyID\n",
    "            PickupLatitude\n",
    "            PickupLongitude\n",
    "            PickupLatLong\n",
    "            DropoffLatitude\n",
    "            DropoffLongitude\n",
    "            DropoffLatLong\n",
    "            PassengerCount\n",
    "            TripDurationSeconds\n",
    "            TripDistanceMiles\n",
    "            PaymentType\n",
    "            FareAmount\n",
    "            SurchargeAmount\n",
    "            TaxAmount\n",
    "            TipAmount\n",
    "            TollsAmount\n",
    "            TotalAmount\n",
    "         }}\n",
    "         endCursor\n",
    "         hasNextPage\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🔄 Fetching trips page (size: {page_size})\")\n",
    "    if after_cursor:\n",
    "        print(f\"   Using cursor: {after_cursor[:20]}...\")\n",
    "    \n",
    "    data = execute_graphql_query(query)\n",
    "    \n",
    "    if data and 'trips' in data:\n",
    "        print_pagination_info(data['trips'], \"trips\")\n",
    "        return data['trips']\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test: Fetch first page of trips\n",
    "print(\"=== SINGLE PAGE FETCH TEST ===\")\n",
    "first_page = fetch_trips_page(page_size=10)\n",
    "\n",
    "if first_page and first_page['items']:\n",
    "    print(f\"\\n📋 Sample trip data:\")\n",
    "    sample_trip = first_page['items'][0]\n",
    "    for key, value in sample_trip.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "else:\n",
    "    print(\"❌ No trip data retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cba50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLETE PAGINATION DEMO ===\n",
      "⚠️  Fetching first 5 pages for demonstration (50 trips max)\n",
      "    In production, remove max_pages limit to fetch all data\n",
      "🚀 Starting complete trip data fetch\n",
      "📋 Configuration:\n",
      "   • Page size: 100\n",
      "   • Max pages: 5\n",
      "   • Delay between requests: 0.5s\n",
      "==================================================\n",
      "\n",
      "📖 Fetching page 1...\n",
      "🔄 Fetching trips page (size: 100)\n",
      "📊 Retrieved 100 trips\n",
      "📄 Has next page: True\n",
      "🔗 End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwMSwiRGlyZWN0aW9uIjowfV0=\n",
      "   ✅ Added 100 trips\n",
      "   📊 Total trips collected: 100\n",
      "\n",
      "📖 Fetching page 2...\n",
      "🔄 Fetching trips page (size: 100)\n",
      "   Using cursor: W3siRW50aXR5TmFtZSI6...\n",
      "📊 Retrieved 100 trips\n",
      "📄 Has next page: True\n",
      "🔗 End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwMiwiRGlyZWN0aW9uIjowfV0=\n",
      "   ✅ Added 100 trips\n",
      "   📊 Total trips collected: 200\n",
      "\n",
      "📖 Fetching page 3...\n",
      "🔄 Fetching trips page (size: 100)\n",
      "   Using cursor: W3siRW50aXR5TmFtZSI6...\n",
      "📊 Retrieved 100 trips\n",
      "📄 Has next page: True\n",
      "🔗 End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwMywiRGlyZWN0aW9uIjowfV0=\n",
      "   ✅ Added 100 trips\n",
      "   📊 Total trips collected: 300\n",
      "\n",
      "📖 Fetching page 4...\n",
      "🔄 Fetching trips page (size: 100)\n",
      "   Using cursor: W3siRW50aXR5TmFtZSI6...\n",
      "📊 Retrieved 100 trips\n",
      "📄 Has next page: True\n",
      "🔗 End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwNCwiRGlyZWN0aW9uIjowfV0=\n",
      "   ✅ Added 100 trips\n",
      "   📊 Total trips collected: 400\n",
      "\n",
      "📖 Fetching page 5...\n",
      "🔄 Fetching trips page (size: 100)\n",
      "   Using cursor: W3siRW50aXR5TmFtZSI6...\n",
      "📊 Retrieved 100 trips\n",
      "📄 Has next page: True\n",
      "🔗 End cursor: W3siRW50aXR5TmFtZSI6IlRyaXAiLCJGaWVsZE5hbWUiOiJEYXRlSUQiLCJGaWVsZFZhbHVlIjoyMDEzMDEwNSwiRGlyZWN0aW9uIjowfV0=\n",
      "   ✅ Added 100 trips\n",
      "   📊 Total trips collected: 500\n",
      "🛑 Reached maximum page limit: 5\n",
      "\n",
      "==================================================\n",
      "🎯 PAGINATION COMPLETE!\n",
      "📊 Final Results:\n",
      "   • Total trips retrieved: 500\n",
      "   • Pages processed: 6\n",
      "   • Time elapsed: 11.60 seconds\n",
      "   • Average time per page: 1.93 seconds\n"
     ]
    }
   ],
   "source": [
    "# 🔄 Complete Pagination: Fetch ALL Trip Data\n",
    "\n",
    "def fetch_all_trips(page_size=100, max_pages=None, delay_seconds=0.5):\n",
    "    \"\"\"\n",
    "    Fetch ALL trip data using pagination with configurable limits\n",
    "    \n",
    "    Args:\n",
    "        page_size (int): Number of items per page\n",
    "        max_pages (int): Maximum pages to fetch (None for unlimited)\n",
    "        delay_seconds (float): Delay between requests to avoid rate limiting\n",
    "    \n",
    "    Returns:\n",
    "        list: All trip records retrieved\n",
    "    \"\"\"\n",
    "    all_trips = []\n",
    "    after_cursor = None\n",
    "    page_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"🚀 Starting complete trip data fetch\")\n",
    "    print(f\"📋 Configuration:\")\n",
    "    print(f\"   • Page size: {page_size}\")\n",
    "    print(f\"   • Max pages: {max_pages or 'Unlimited'}\")\n",
    "    print(f\"   • Delay between requests: {delay_seconds}s\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    while True:\n",
    "        page_count += 1\n",
    "        \n",
    "        # Check page limit\n",
    "        if max_pages and page_count > max_pages:\n",
    "            print(f\"🛑 Reached maximum page limit: {max_pages}\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n📖 Fetching page {page_count}...\")\n",
    "        \n",
    "        # Fetch current page\n",
    "        page_data = fetch_trips_page(page_size=page_size, after_cursor=after_cursor)\n",
    "        \n",
    "        if not page_data or not page_data.get('items'):\n",
    "            print(f\"ℹ️  No more data found at page {page_count}\")\n",
    "            break\n",
    "        \n",
    "        # Add items to collection\n",
    "        items = page_data['items']\n",
    "        all_trips.extend(items)\n",
    "        \n",
    "        print(f\"   ✅ Added {len(items)} trips\")\n",
    "        print(f\"   📊 Total trips collected: {len(all_trips)}\")\n",
    "        \n",
    "        # Check if there's a next page\n",
    "        if not page_data.get('hasNextPage', False):\n",
    "            print(f\"ℹ️  Reached end of data (no more pages)\")\n",
    "            break\n",
    "        \n",
    "        # Update cursor for next page\n",
    "        after_cursor = page_data.get('endCursor')\n",
    "        \n",
    "        # Rate limiting delay\n",
    "        if delay_seconds > 0:\n",
    "            time.sleep(delay_seconds)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"🎯 PAGINATION COMPLETE!\")\n",
    "    print(f\"📊 Final Results:\")\n",
    "    print(f\"   • Total trips retrieved: {len(all_trips):,}\")\n",
    "    print(f\"   • Pages processed: {page_count}\")\n",
    "    print(f\"   • Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"   • Average time per page: {elapsed_time/page_count:.2f} seconds\")\n",
    "    \n",
    "    return all_trips\n",
    "\n",
    "# Execute complete pagination with reasonable limits for demo\n",
    "print(\"=== COMPLETE PAGINATION DEMO ===\")\n",
    "print(\"⚠️  Fetching first 5 pages for demonstration (50 trips max)\")\n",
    "print(\"    In production, remove max_pages limit to fetch all data\")\n",
    "\n",
    "all_trip_data = fetch_all_trips(\n",
    "    page_size=100,\n",
    "    max_pages=5,  # Limit for demo - remove this in production\n",
    "    delay_seconds=0.5  # Small delay to be respectful to the API\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c52afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 EXPORTING TRIP DATA TO JSON\n",
      "==================================================\n",
      "✅ Trip data successfully exported!\n",
      "📁 File: fabric_trips_data_20250924_102403.json\n",
      "📊 Records: 500\n",
      "💾 File size: 372,022 bytes (0.35 MB)\n",
      "\n",
      "📋 Export Summary:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m saved_file:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m📋 Export Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   • File location: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mos\u001b[49m.path.abspath(saved_file)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   • Data structure: JSON with metadata wrapper\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   • Encoding: UTF-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 💾 Export Trip Data to JSON File\n",
    "\n",
    "def save_trips_to_json(trips_data, filename=None):\n",
    "    \"\"\"\n",
    "    Save trip data to a JSON file with metadata\n",
    "    \n",
    "    Args:\n",
    "        trips_data (list): List of trip records\n",
    "        filename (str): Optional filename, auto-generated if not provided\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the saved file\n",
    "    \"\"\"\n",
    "    if not trips_data:\n",
    "        print(\"❌ No trip data to save\")\n",
    "        return None\n",
    "    \n",
    "    # Generate filename if not provided\n",
    "    if not filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"fabric_trips_data_{timestamp}.json\"\n",
    "    \n",
    "    # Create export data with metadata\n",
    "    export_data = {\n",
    "        \"metadata\": {\n",
    "            \"export_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_records\": len(trips_data),\n",
    "            \"source\": \"Microsoft Fabric GraphQL API\",\n",
    "            \"schema_version\": \"1.0\"\n",
    "        },\n",
    "        \"trips\": trips_data\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Write to JSON file with proper formatting\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(export_data, file, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        # Get file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename)\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"✅ Trip data successfully exported!\")\n",
    "        print(f\"📁 File: {filename}\")\n",
    "        print(f\"📊 Records: {len(trips_data):,}\")\n",
    "        print(f\"💾 File size: {file_size:,} bytes ({file_size_mb:.2f} MB)\")\n",
    "        \n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Save the fetched trip data to JSON file\n",
    "if 'all_trip_data' in locals() and all_trip_data:\n",
    "    print(\"💾 EXPORTING TRIP DATA TO JSON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    saved_file = save_trips_to_json(all_trip_data)\n",
    "    \n",
    "    if saved_file:\n",
    "        print(f\"\\n📋 Export Summary:\")\n",
    "        print(f\"   • File location: {os.path.abspath(saved_file)}\")\n",
    "        print(f\"   • Data structure: JSON with metadata wrapper\")\n",
    "        print(f\"   • Encoding: UTF-8\")\n",
    "        print(f\"   • Format: Pretty-printed with 2-space indentation\")\n",
    "        \n",
    "        # Show sample structure\n",
    "        print(f\"\\n🔍 File Structure Preview:\")\n",
    "        print(f\"{{\")\n",
    "        print(f\"  'metadata': {{\")\n",
    "        print(f\"    'export_timestamp': '2025-09-24T...',\")\n",
    "        print(f\"    'total_records': {len(all_trip_data)},\")\n",
    "        print(f\"    'source': 'Microsoft Fabric GraphQL API',\")\n",
    "        print(f\"    'schema_version': '1.0'\")\n",
    "        print(f\"  }},\")\n",
    "        print(f\"  'trips': [\")\n",
    "        print(f\"    {{ trip_record_1 }},\")\n",
    "        print(f\"    {{ trip_record_2 }},\")\n",
    "        print(f\"    ...\")\n",
    "        print(f\"  ]\")\n",
    "        print(f\"}}\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  No trip data available to export\")\n",
    "    print(\"   Run the pagination cells above first to fetch trip data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75faec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 Advanced Export Options\n",
    "\n",
    "def export_trips_multiple_formats(trips_data, base_filename=None):\n",
    "    \"\"\"\n",
    "    Export trip data in multiple formats: JSON, CSV, and summary\n",
    "    \n",
    "    Args:\n",
    "        trips_data (list): List of trip records\n",
    "        base_filename (str): Base filename (without extension)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with paths to all created files\n",
    "    \"\"\"\n",
    "    if not trips_data:\n",
    "        print(\"❌ No trip data to export\")\n",
    "        return {}\n",
    "    \n",
    "    # Generate base filename if not provided\n",
    "    if not base_filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_filename = f\"fabric_trips_{timestamp}\"\n",
    "    \n",
    "    created_files = {}\n",
    "    \n",
    "    print(f\"📂 EXPORTING TO MULTIPLE FORMATS\")\n",
    "    print(f\"🏷️  Base filename: {base_filename}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Export as JSON (detailed format)\n",
    "    try:\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        export_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_records\": len(trips_data),\n",
    "                \"source\": \"Microsoft Fabric GraphQL API\",\n",
    "                \"export_format\": \"detailed_json\",\n",
    "                \"columns\": list(trips_data[0].keys()) if trips_data else [],\n",
    "                \"schema_version\": \"1.0\"\n",
    "            },\n",
    "            \"trips\": trips_data\n",
    "        }\n",
    "        \n",
    "        with open(json_filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(export_data, file, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        created_files['json'] = json_filename\n",
    "        file_size = os.path.getsize(json_filename) / (1024 * 1024)\n",
    "        print(f\"✅ JSON: {json_filename} ({file_size:.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ JSON export failed: {str(e)}\")\n",
    "    \n",
    "    # 2. Export as CSV (for Excel compatibility)\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        \n",
    "        df = pd.DataFrame(trips_data)\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        created_files['csv'] = csv_filename\n",
    "        file_size = os.path.getsize(csv_filename) / (1024 * 1024)\n",
    "        print(f\"✅ CSV: {csv_filename} ({file_size:.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CSV export failed: {str(e)}\")\n",
    "    \n",
    "    # 3. Export summary statistics\n",
    "    try:\n",
    "        summary_filename = f\"{base_filename}_summary.json\"\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        df = pd.DataFrame(trips_data)\n",
    "        \n",
    "        # Numeric columns summary\n",
    "        numeric_summary = {}\n",
    "        numeric_cols = df.select_dtypes(include=[int, float]).columns\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                numeric_summary[col] = {\n",
    "                    \"count\": int(df[col].count()),\n",
    "                    \"mean\": float(df[col].mean()) if pd.notna(df[col].mean()) else None,\n",
    "                    \"std\": float(df[col].std()) if pd.notna(df[col].std()) else None,\n",
    "                    \"min\": float(df[col].min()) if pd.notna(df[col].min()) else None,\n",
    "                    \"max\": float(df[col].max()) if pd.notna(df[col].max()) else None\n",
    "                }\n",
    "        \n",
    "        # Categorical columns summary\n",
    "        categorical_summary = {}\n",
    "        categorical_cols = df.select_dtypes(include=[object]).columns\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                value_counts = df[col].value_counts().head(10).to_dict()\n",
    "                categorical_summary[col] = {\n",
    "                    \"unique_count\": int(df[col].nunique()),\n",
    "                    \"top_values\": {str(k): int(v) for k, v in value_counts.items()}\n",
    "                }\n",
    "        \n",
    "        summary_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_timestamp\": datetime.now().isoformat(),\n",
    "                \"source_records\": len(trips_data),\n",
    "                \"analysis_type\": \"summary_statistics\"\n",
    "            },\n",
    "            \"dataset_overview\": {\n",
    "                \"total_records\": len(df),\n",
    "                \"total_columns\": len(df.columns),\n",
    "                \"numeric_columns\": len(numeric_cols),\n",
    "                \"categorical_columns\": len(categorical_cols),\n",
    "                \"memory_usage_mb\": round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2)\n",
    "            },\n",
    "            \"numeric_statistics\": numeric_summary,\n",
    "            \"categorical_statistics\": categorical_summary\n",
    "        }\n",
    "        \n",
    "        with open(summary_filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(summary_data, file, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        created_files['summary'] = summary_filename\n",
    "        file_size = os.path.getsize(summary_filename) / 1024  # KB for summary\n",
    "        print(f\"✅ Summary: {summary_filename} ({file_size:.1f} KB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Summary export failed: {str(e)}\")\n",
    "    \n",
    "    # 4. Create a manifest file\n",
    "    try:\n",
    "        manifest_filename = f\"{base_filename}_manifest.json\"\n",
    "        \n",
    "        manifest_data = {\n",
    "            \"export_info\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"base_filename\": base_filename,\n",
    "                \"source\": \"Microsoft Fabric GraphQL API\",\n",
    "                \"total_records\": len(trips_data)\n",
    "            },\n",
    "            \"files_created\": created_files,\n",
    "            \"file_descriptions\": {\n",
    "                \"json\": \"Complete trip data with metadata in JSON format\",\n",
    "                \"csv\": \"Trip data in CSV format for Excel/analysis tools\",\n",
    "                \"summary\": \"Statistical summary and data profiling information\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(manifest_filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(manifest_data, file, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        created_files['manifest'] = manifest_filename\n",
    "        print(f\"✅ Manifest: {manifest_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Manifest creation failed: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"🎉 Export complete! Created {len(created_files)} files\")\n",
    "    \n",
    "    return created_files\n",
    "\n",
    "# Execute the multi-format export\n",
    "if 'all_trip_data' in locals() and all_trip_data:\n",
    "    print(\"🚀 EXECUTING MULTI-FORMAT EXPORT\")\n",
    "    \n",
    "    export_results = export_trips_multiple_formats(all_trip_data)\n",
    "    \n",
    "    if export_results:\n",
    "        print(f\"\\n📁 All files created in current directory:\")\n",
    "        for format_type, filename in export_results.items():\n",
    "            full_path = os.path.abspath(filename)\n",
    "            print(f\"   {format_type.upper()}: {full_path}\")\n",
    "        \n",
    "        print(f\"\\n💡 Usage Tips:\")\n",
    "        print(f\"   • Open .csv file in Excel for data analysis\")\n",
    "        print(f\"   • Use .json file for programmatic processing\")\n",
    "        print(f\"   • Check _summary.json for data insights\")\n",
    "        print(f\"   • Review _manifest.json for export details\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  No trip data available for export\")\n",
    "    print(\"   Execute the pagination cells first to fetch data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Data Analysis and Summary\n",
    "\n",
    "if all_trip_data:\n",
    "    print(\"=== TRIP DATA ANALYSIS ===\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df_trips = pd.DataFrame(all_trip_data)\n",
    "    \n",
    "    print(f\"📋 Dataset Overview:\")\n",
    "    print(f\"   • Total trips: {len(df_trips):,}\")\n",
    "    print(f\"   • Columns: {len(df_trips.columns)}\")\n",
    "    print(f\"   • Data types:\")\n",
    "    \n",
    "    # Show data types\n",
    "    for col in df_trips.columns:\n",
    "        dtype = df_trips[col].dtype\n",
    "        non_null = df_trips[col].notna().sum()\n",
    "        print(f\"     - {col}: {dtype} ({non_null}/{len(df_trips)} non-null)\")\n",
    "    \n",
    "    print(f\"\\n💰 Financial Summary:\")\n",
    "    if 'FareAmount' in df_trips.columns:\n",
    "        fare_amounts = pd.to_numeric(df_trips['FareAmount'], errors='coerce')\n",
    "        total_amounts = pd.to_numeric(df_trips['TotalAmount'], errors='coerce')\n",
    "        \n",
    "        print(f\"   • Total Fare Amount: ${fare_amounts.sum():,.2f}\")\n",
    "        print(f\"   • Average Fare: ${fare_amounts.mean():.2f}\")\n",
    "        print(f\"   • Total Amount (with tips/taxes): ${total_amounts.sum():,.2f}\")\n",
    "        print(f\"   • Average Total: ${total_amounts.mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\n🚗 Trip Summary:\")\n",
    "    if 'PassengerCount' in df_trips.columns:\n",
    "        passenger_counts = pd.to_numeric(df_trips['PassengerCount'], errors='coerce')\n",
    "        print(f\"   • Total Passengers: {passenger_counts.sum():,}\")\n",
    "        print(f\"   • Average Passengers per Trip: {passenger_counts.mean():.1f}\")\n",
    "    \n",
    "    if 'TripDistanceMiles' in df_trips.columns:\n",
    "        distances = pd.to_numeric(df_trips['TripDistanceMiles'], errors='coerce')\n",
    "        print(f\"   • Total Distance: {distances.sum():,.1f} miles\")\n",
    "        print(f\"   • Average Distance: {distances.mean():.1f} miles\")\n",
    "    \n",
    "    print(f\"\\n💳 Payment Type Distribution:\")\n",
    "    if 'PaymentType' in df_trips.columns:\n",
    "        payment_counts = df_trips['PaymentType'].value_counts()\n",
    "        for payment_type, count in payment_counts.items():\n",
    "            percentage = (count / len(df_trips)) * 100\n",
    "            print(f\"   • {payment_type}: {count} trips ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n📋 Sample Records (first 3 trips):\")\n",
    "    print(df_trips.head(3).to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trip data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Trip Data Visualization\n",
    "\n",
    "if all_trip_data and len(all_trip_data) > 0:\n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Trip Data Analysis Dashboard', fontsize=16, y=1.02)\n",
    "    \n",
    "    # 1. Fare Amount Distribution\n",
    "    if 'FareAmount' in df_trips.columns:\n",
    "        fare_amounts = pd.to_numeric(df_trips['FareAmount'], errors='coerce').dropna()\n",
    "        if len(fare_amounts) > 0:\n",
    "            axes[0, 0].hist(fare_amounts, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[0, 0].set_title('Fare Amount Distribution')\n",
    "            axes[0, 0].set_xlabel('Fare Amount ($)')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].axvline(fare_amounts.mean(), color='red', linestyle='--', \n",
    "                              label=f'Mean: ${fare_amounts.mean():.2f}')\n",
    "            axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Payment Type Distribution\n",
    "    if 'PaymentType' in df_trips.columns:\n",
    "        payment_counts = df_trips['PaymentType'].value_counts()\n",
    "        if len(payment_counts) > 0:\n",
    "            axes[0, 1].pie(payment_counts.values, labels=payment_counts.index, autopct='%1.1f%%')\n",
    "            axes[0, 1].set_title('Payment Type Distribution')\n",
    "    \n",
    "    # 3. Trip Distance Distribution\n",
    "    if 'TripDistanceMiles' in df_trips.columns:\n",
    "        distances = pd.to_numeric(df_trips['TripDistanceMiles'], errors='coerce').dropna()\n",
    "        if len(distances) > 0:\n",
    "            axes[1, 0].hist(distances, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "            axes[1, 0].set_title('Trip Distance Distribution')\n",
    "            axes[1, 0].set_xlabel('Distance (miles)')\n",
    "            axes[1, 0].set_ylabel('Frequency')\n",
    "            axes[1, 0].axvline(distances.mean(), color='red', linestyle='--', \n",
    "                              label=f'Mean: {distances.mean():.2f} miles')\n",
    "            axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Passenger Count Distribution\n",
    "    if 'PassengerCount' in df_trips.columns:\n",
    "        passenger_counts = df_trips['PassengerCount'].value_counts().sort_index()\n",
    "        if len(passenger_counts) > 0:\n",
    "            axes[1, 1].bar(passenger_counts.index, passenger_counts.values, \n",
    "                          alpha=0.7, color='orange', edgecolor='black')\n",
    "            axes[1, 1].set_title('Passenger Count Distribution')\n",
    "            axes[1, 1].set_xlabel('Number of Passengers')\n",
    "            axes[1, 1].set_ylabel('Number of Trips')\n",
    "            axes[1, 1].set_xticks(passenger_counts.index)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create correlation matrix if we have numeric columns\n",
    "    numeric_cols = df_trips.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = df_trips[numeric_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5)\n",
    "        plt.title('Trip Data Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bb7e1",
   "metadata": {},
   "source": [
    "## 🚀 Advanced Pagination Techniques\n",
    "\n",
    "### Handling Large Datasets\n",
    "\n",
    "When working with large datasets through GraphQL pagination, consider these best practices:\n",
    "\n",
    "1. **Rate Limiting**: Add delays between requests to avoid overwhelming the API\n",
    "2. **Error Handling**: Implement robust retry logic for network failures\n",
    "3. **Memory Management**: Process data in chunks to avoid memory issues\n",
    "4. **Progress Tracking**: Show progress for long-running operations\n",
    "5. **Caching**: Store intermediate results to resume interrupted operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Production-Ready Pagination with Error Handling\n",
    "\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Generator\n",
    "\n",
    "def fetch_all_trips_robust(\n",
    "    page_size: int = 10,\n",
    "    max_total_records: Optional[int] = None,\n",
    "    delay_between_requests: float = 0.5,\n",
    "    max_retries: int = 3\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Robust pagination implementation with error handling and rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        page_size: Number of records per page\n",
    "        max_total_records: Maximum total records to fetch (None = all)\n",
    "        delay_between_requests: Seconds to wait between API calls\n",
    "        max_retries: Maximum retry attempts for failed requests\n",
    "    \n",
    "    Returns:\n",
    "        List of all trip records\n",
    "    \"\"\"\n",
    "    all_trips = []\n",
    "    cursor = None\n",
    "    page_number = 1\n",
    "    total_fetched = 0\n",
    "    \n",
    "    print(f\"🚀 Starting robust pagination (page_size={page_size}, max_records={max_total_records})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        retry_count = 0\n",
    "        current_page_data = None\n",
    "        \n",
    "        # Retry logic for current page\n",
    "        while retry_count <= max_retries:\n",
    "            try:\n",
    "                print(f\"📄 Fetching page {page_number} (attempt {retry_count + 1}/{max_retries + 1})\")\n",
    "                \n",
    "                current_page_data = fetch_trips_page(\n",
    "                    page_size=page_size, \n",
    "                    cursor=cursor\n",
    "                )\n",
    "                \n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"⚠️ Error on page {page_number}, attempt {retry_count}: {str(e)}\")\n",
    "                \n",
    "                if retry_count <= max_retries:\n",
    "                    wait_time = 2 ** retry_count  # Exponential backoff\n",
    "                    print(f\"⏳ Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"❌ Failed to fetch page {page_number} after {max_retries + 1} attempts\")\n",
    "                    print(f\"📊 Returning {len(all_trips)} trips fetched so far\")\n",
    "                    return all_trips\n",
    "        \n",
    "        # Process successful page\n",
    "        if current_page_data and current_page_data.get('items'):\n",
    "            trips_on_page = current_page_data['items']\n",
    "            all_trips.extend(trips_on_page)\n",
    "            total_fetched += len(trips_on_page)\n",
    "            \n",
    "            print(f\"✅ Page {page_number}: {len(trips_on_page)} trips fetched\")\n",
    "            print(f\"📊 Total so far: {total_fetched} trips\")\n",
    "            \n",
    "            # Check if we've reached the maximum limit\n",
    "            if max_total_records and total_fetched >= max_total_records:\n",
    "                all_trips = all_trips[:max_total_records]  # Trim to exact limit\n",
    "                print(f\"🏁 Reached maximum limit of {max_total_records} records\")\n",
    "                break\n",
    "            \n",
    "            # Check if there are more pages\n",
    "            if not current_page_data.get('hasNextPage', False):\n",
    "                print(f\"🏁 No more pages available\")\n",
    "                break\n",
    "            \n",
    "            # Prepare for next page\n",
    "            cursor = current_page_data.get('endCursor')\n",
    "            if not cursor:\n",
    "                print(f\"⚠️ No cursor for next page, stopping\")\n",
    "                break\n",
    "            \n",
    "            page_number += 1\n",
    "            \n",
    "            # Rate limiting\n",
    "            if delay_between_requests > 0:\n",
    "                print(f\"⏱️ Waiting {delay_between_requests}s before next request...\")\n",
    "                time.sleep(delay_between_requests)\n",
    "        else:\n",
    "            print(f\"❌ No data returned for page {page_number}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🎉 Pagination complete! Total trips fetched: {len(all_trips)}\")\n",
    "    return all_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5766da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Test the Robust Pagination (Limited to 50 records for demo)\n",
    "\n",
    "print(\"🧪 TESTING ROBUST PAGINATION\")\n",
    "print(\"This demo fetches 50 records to show the robust pagination in action...\")\n",
    "print()\n",
    "\n",
    "# Test with limited records for demonstration\n",
    "test_trips = fetch_all_trips_robust(\n",
    "    page_size=10,\n",
    "    max_total_records=50,  # Limit for demo\n",
    "    delay_between_requests=0.1,  # Minimal delay for demo\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "if test_trips:\n",
    "    print(f\"\\n🎯 RESULTS SUMMARY:\")\n",
    "    print(f\"   ✅ Successfully fetched {len(test_trips)} trip records\")\n",
    "    print(f\"   📄 Pages processed: {(len(test_trips) - 1) // 10 + 1}\")\n",
    "    print(f\"   🔍 Sample trip IDs: {[trip.get('TripId', 'N/A') for trip in test_trips[:5]]}\")\n",
    "else:\n",
    "    print(\"\\n❌ No trips were fetched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b4ab4",
   "metadata": {},
   "source": [
    "## 📚 Summary & Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "✅ **Authentication Setup**: Configured Microsoft Fabric GraphQL API access  \n",
    "✅ **Schema Understanding**: Analyzed the Trip data structure with pagination support  \n",
    "✅ **Basic Pagination**: Implemented simple page-by-page data fetching  \n",
    "✅ **Complete Pagination**: Created functions to fetch ALL data using cursor-based pagination  \n",
    "✅ **Data Analysis**: Added comprehensive data analysis and visualization  \n",
    "✅ **Production-Ready Code**: Implemented robust error handling and rate limiting  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Page Size**: Use `first: 10` parameter to control page size\n",
    "2. **Cursor Navigation**: Use `after: \"cursor_value\"` for next pages\n",
    "3. **Completion Detection**: Check `hasNextPage` to know when to stop\n",
    "4. **Error Handling**: Always implement retry logic for production systems\n",
    "5. **Rate Limiting**: Add delays between requests to be API-friendly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Scale Up**: Remove the `max_total_records` limit to fetch complete dataset\n",
    "- **Data Pipeline**: Export results to CSV/database for further analysis\n",
    "- **Monitoring**: Add logging and metrics for production deployments\n",
    "- **Optimization**: Implement parallel processing for multiple data streams\n",
    "\n",
    "### 🎯 Ready to Use!\n",
    "\n",
    "Your pagination implementation is now ready to handle real-world scenarios with:\n",
    "- Configurable page sizes\n",
    "- Robust error handling  \n",
    "- Progress tracking\n",
    "- Memory-efficient processing\n",
    "- Production-grade reliability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
