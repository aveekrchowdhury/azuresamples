{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks Event Hub Producer - Optimized for Spark DataFrames\n",
    "import os, json, gzip, time, uuid, random\n",
    "from typing import Iterable, List, Dict, Any\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "from azure.eventhub.exceptions import EventHubError, OperationTimeoutError\n",
    "from pyspark.sql.functions import col, to_json, struct\n",
    "from pyspark.sql.types import StringType\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Databricks-specific Configuration ---\n",
    "# Event Hub connection details from Databricks secrets\n",
    "try:\n",
    "    EVENT_HUB_CONNECTION_STRING = dbutils.secrets.get(\"kv-scope\", \"eh-connstr\")\n",
    "    EVENT_HUB_NAME = dbutils.secrets.get(\"kv-scope\", \"eh-name\") or \"datatransferhub\"\n",
    "    logger.info(\"✅ Successfully retrieved Event Hub credentials from secrets\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"⚠️ Could not retrieve secrets: {e}\")\n",
    "    # Fallback for testing\n",
    "    EVENT_HUB_CONNECTION_STRING = \"YOUR_CONNECTION_STRING_HERE\"\n",
    "    EVENT_HUB_NAME = \"datatransferhub\"\n",
    "\n",
    "# --- Constants for Databricks/Event Hub ---\n",
    "MAX_EVENT_BYTES = 900_000      # ~0.9 MB safety margin (1MB limit - overhead)\n",
    "MAX_RETRIES = 7                # Retry attempts for transient failures\n",
    "BASE_DELAY = 0.5               # Base delay for exponential backoff\n",
    "MAX_BACKOFF = 30               # Maximum backoff delay\n",
    "BATCH_SIZE = 100               # Events per batch for efficient processing\n",
    "PARTITION_COUNT = 32           # Number of Event Hub partitions (adjust to your setup)\n",
    "\n",
    "print(f\"🔧 Configuration:\")\n",
    "print(f\"   Event Hub: {EVENT_HUB_NAME}\")\n",
    "print(f\"   Max Event Size: {MAX_EVENT_BYTES:,} bytes\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE} events\")\n",
    "print(f\"   Partition Count: {PARTITION_COUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885263f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions for Databricks + Event Hub ---\n",
    "\n",
    "def to_bytes(obj) -> bytes:\n",
    "    \"\"\"Convert various data types to bytes for Event Hub.\"\"\"\n",
    "    if isinstance(obj, (bytes, bytearray)):\n",
    "        return bytes(obj)\n",
    "    if isinstance(obj, str):\n",
    "        return obj.encode(\"utf-8\")\n",
    "    if isinstance(obj, dict):\n",
    "        return json.dumps(obj, separators=(\",\", \":\"), ensure_ascii=False).encode(\"utf-8\")\n",
    "    return json.dumps(obj, separators=(\",\", \":\"), ensure_ascii=False, default=str).encode(\"utf-8\")\n",
    "\n",
    "def gzip_compress(data: bytes) -> bytes:\n",
    "    \"\"\"Compress data using gzip for efficient transport.\"\"\"\n",
    "    return gzip.compress(data)\n",
    "\n",
    "def chunk_data(data: bytes, max_size: int) -> List[bytes]:\n",
    "    \"\"\"Split large data into chunks that fit Event Hub size limits.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(data), max_size):\n",
    "        chunks.append(data[i:i + max_size])\n",
    "    return chunks\n",
    "\n",
    "def calculate_partition_key(record: Dict[str, Any], strategy: str = \"hash\") -> str:\n",
    "    \"\"\"Calculate partition key for even distribution across Event Hub partitions.\"\"\"\n",
    "    if strategy == \"hash\":\n",
    "        # Use hash of customer_id or order_id for even distribution\n",
    "        key_field = record.get(\"customer_id\") or record.get(\"order_id\") or record.get(\"id\") or str(uuid.uuid4())\n",
    "        return str(hash(str(key_field)) % PARTITION_COUNT)\n",
    "    elif strategy == \"round_robin\":\n",
    "        # Simple round-robin (requires external counter)\n",
    "        return str(random.randint(0, PARTITION_COUNT - 1))\n",
    "    elif strategy == \"customer_id\":\n",
    "        # Keep all events for same customer on same partition\n",
    "        customer_id = record.get(\"customer_id\", \"unknown\")\n",
    "        return str(hash(str(customer_id)) % PARTITION_COUNT)\n",
    "    else:\n",
    "        return \"0\"  # Default partition\n",
    "\n",
    "def send_batch_with_retry(producer: EventHubProducerClient, batch, max_retries: int = MAX_RETRIES):\n",
    "    \"\"\"Send batch with exponential backoff retry logic.\"\"\"\n",
    "    delay = BASE_DELAY\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            producer.send_batch(batch)\n",
    "            logger.info(f\"✅ Successfully sent batch with {batch.count} events\")\n",
    "            return True\n",
    "            \n",
    "        except (EventHubError, OperationTimeoutError) as e:\n",
    "            if attempt == max_retries:\n",
    "                logger.error(f\"❌ Failed to send batch after {max_retries} attempts: {e}\")\n",
    "                raise\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            jitter = random.uniform(0, delay * 0.25)\n",
    "            sleep_time = min(delay + jitter, MAX_BACKOFF)\n",
    "            logger.warning(f\"⚠️ Attempt {attempt} failed, retrying in {sleep_time:.2f}s: {e}\")\n",
    "            time.sleep(sleep_time)\n",
    "            delay = min(delay * 2, MAX_BACKOFF)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Unexpected error sending batch: {e}\")\n",
    "            raise\n",
    "    \n",
    "    return False\n",
    "\n",
    "print(\"✅ Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8dcbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Databricks Spark DataFrame Processing ---\n",
    "\n",
    "class EventHubSender:\n",
    "    \"\"\"\n",
    "    Databricks-optimized Event Hub sender with batching and partitioning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string: str, event_hub_name: str):\n",
    "        self.connection_string = connection_string\n",
    "        self.event_hub_name = event_hub_name\n",
    "        self.producer = None\n",
    "        self.stats = {\n",
    "            \"events_sent\": 0,\n",
    "            \"batches_sent\": 0,\n",
    "            \"chunks_created\": 0,\n",
    "            \"errors\": 0\n",
    "        }\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Initialize Event Hub producer client.\"\"\"\n",
    "        try:\n",
    "            self.producer = EventHubProducerClient.from_connection_string(\n",
    "                conn_str=self.connection_string,\n",
    "                eventhub_name=self.event_hub_name\n",
    "            )\n",
    "            logger.info(f\"🔗 Connected to Event Hub: {self.event_hub_name}\")\n",
    "            return self\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to connect to Event Hub: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Close Event Hub producer and print statistics.\"\"\"\n",
    "        if self.producer:\n",
    "            self.producer.close()\n",
    "            logger.info(\"🔌 Event Hub connection closed\")\n",
    "        \n",
    "        print(\"\\n📊 Event Hub Sending Statistics:\")\n",
    "        print(f\"   Events sent: {self.stats['events_sent']:,}\")\n",
    "        print(f\"   Batches sent: {self.stats['batches_sent']:,}\")\n",
    "        print(f\"   Chunks created: {self.stats['chunks_created']:,}\")\n",
    "        print(f\"   Errors: {self.stats['errors']:,}\")\n",
    "    \n",
    "    def create_event_from_record(self, record: Dict[str, Any], correlation_id: str) -> List[EventData]:\n",
    "        \"\"\"\n",
    "        Create Event Hub events from a single record, with chunking if needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert record to JSON and compress\n",
    "            json_data = json.dumps(record, ensure_ascii=False, default=str)\n",
    "            compressed_data = gzip_compress(to_bytes(json_data))\n",
    "            \n",
    "            # Split into chunks if too large\n",
    "            chunks = chunk_data(compressed_data, MAX_EVENT_BYTES)\n",
    "            events = []\n",
    "            \n",
    "            for chunk_idx, chunk in enumerate(chunks):\n",
    "                event = EventData(chunk)\n",
    "                event.content_type = \"application/json+gzip\"\n",
    "                event.properties = {\n",
    "                    \"correlation_id\": correlation_id,\n",
    "                    \"chunk_index\": chunk_idx + 1,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"compressed\": True,\n",
    "                    \"schema_version\": \"v1\",\n",
    "                    \"source\": \"databricks\",\n",
    "                    \"timestamp\": int(time.time() * 1000)  # milliseconds\n",
    "                }\n",
    "                events.append(event)\n",
    "            \n",
    "            if len(chunks) > 1:\n",
    "                self.stats[\"chunks_created\"] += len(chunks)\n",
    "                logger.debug(f\"📦 Split large record into {len(chunks)} chunks\")\n",
    "            \n",
    "            return events\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error creating event from record: {e}\")\n",
    "            self.stats[\"errors\"] += 1\n",
    "            return []\n",
    "    \n",
    "    def send_records_batch(self, records: List[Dict[str, Any]], partition_strategy: str = \"hash\"):\n",
    "        \"\"\"\n",
    "        Send a batch of records to Event Hub with optimal partitioning.\n",
    "        \"\"\"\n",
    "        if not records:\n",
    "            return\n",
    "        \n",
    "        # Group records by partition key for efficient batching\n",
    "        partition_groups = {}\n",
    "        \n",
    "        for record in records:\n",
    "            correlation_id = str(uuid.uuid4())\n",
    "            partition_key = calculate_partition_key(record, partition_strategy)\n",
    "            \n",
    "            if partition_key not in partition_groups:\n",
    "                partition_groups[partition_key] = []\n",
    "            \n",
    "            events = self.create_event_from_record(record, correlation_id)\n",
    "            if events:\n",
    "                partition_groups[partition_key].extend(events)\n",
    "        \n",
    "        # Send each partition group as separate batches\n",
    "        for partition_key, events in partition_groups.items():\n",
    "            self._send_events_for_partition(events, partition_key)\n",
    "    \n",
    "    def _send_events_for_partition(self, events: List[EventData], partition_key: str):\n",
    "        \"\"\"Send events for a specific partition, handling batch size limits.\"\"\"\n",
    "        current_batch = None\n",
    "        \n",
    "        try:\n",
    "            current_batch = self.producer.create_batch(partition_key=partition_key)\n",
    "            \n",
    "            for event in events:\n",
    "                try:\n",
    "                    current_batch.add(event)\n",
    "                except ValueError:\n",
    "                    # Batch is full, send it and create a new one\n",
    "                    if current_batch.count > 0:\n",
    "                        send_batch_with_retry(self.producer, current_batch)\n",
    "                        self.stats[\"batches_sent\"] += 1\n",
    "                        self.stats[\"events_sent\"] += current_batch.count\n",
    "                    \n",
    "                    # Create new batch and add the event\n",
    "                    current_batch = self.producer.create_batch(partition_key=partition_key)\n",
    "                    current_batch.add(event)\n",
    "            \n",
    "            # Send final batch if it has events\n",
    "            if current_batch and current_batch.count > 0:\n",
    "                send_batch_with_retry(self.producer, current_batch)\n",
    "                self.stats[\"batches_sent\"] += 1\n",
    "                self.stats[\"events_sent\"] += current_batch.count\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error sending events for partition {partition_key}: {e}\")\n",
    "            self.stats[\"errors\"] += 1\n",
    "\n",
    "print(\"✅ EventHubSender class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Databricks to Event Hub Processing ---\n",
    "\n",
    "def process_dataframe_to_eventhub(\n",
    "    df, \n",
    "    connection_string: str, \n",
    "    event_hub_name: str,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    partition_strategy: str = \"hash\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a Spark DataFrame and send to Event Hub efficiently.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to process\n",
    "        connection_string: Event Hub connection string\n",
    "        event_hub_name: Event Hub name\n",
    "        batch_size: Number of records to process in each batch\n",
    "        partition_strategy: Partitioning strategy ('hash', 'customer_id', 'round_robin')\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting DataFrame to Event Hub processing...\")\n",
    "    print(f\"   Strategy: {partition_strategy}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    # Get total count for progress tracking\n",
    "    total_records = df.count()\n",
    "    print(f\"   Total records: {total_records:,}\")\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(\"⚠️ DataFrame is empty, nothing to process\")\n",
    "        return\n",
    "    \n",
    "    # Convert DataFrame to JSON strings for easier processing\n",
    "    # This ensures all columns are properly serialized\n",
    "    df_json = df.select(to_json(struct(*df.columns)).alias(\"json_data\"))\n",
    "    \n",
    "    processed_records = 0\n",
    "    \n",
    "    with EventHubSender(connection_string, event_hub_name) as sender:\n",
    "        \n",
    "        # Process in batches using toLocalIterator for memory efficiency\n",
    "        batch_records = []\n",
    "        \n",
    "        for row in df_json.toLocalIterator():\n",
    "            try:\n",
    "                # Parse JSON back to dict for processing\n",
    "                record = json.loads(row[\"json_data\"])\n",
    "                batch_records.append(record)\n",
    "                \n",
    "                # Send batch when it reaches the desired size\n",
    "                if len(batch_records) >= batch_size:\n",
    "                    sender.send_records_batch(batch_records, partition_strategy)\n",
    "                    processed_records += len(batch_records)\n",
    "                    \n",
    "                    # Progress update\n",
    "                    progress = (processed_records / total_records) * 100\n",
    "                    print(f\"📊 Progress: {processed_records:,}/{total_records:,} ({progress:.1f}%)\")\n",
    "                    \n",
    "                    batch_records = []  # Reset batch\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Error processing record: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Send remaining records in final batch\n",
    "        if batch_records:\n",
    "            sender.send_records_batch(batch_records, partition_strategy)\n",
    "            processed_records += len(batch_records)\n",
    "        \n",
    "        print(f\"✅ Processing complete! Sent {processed_records:,} records to Event Hub\")\n",
    "\n",
    "print(\"✅ Main processing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d87fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage: Sales Customer Data to Event Hub ---\n",
    "\n",
    "# Example 1: Basic usage with your sales customer data\n",
    "print(\"🔧 Example 1: Basic Sales Customer Processing\")\n",
    "\n",
    "try:\n",
    "    # Load your Spark DataFrame (adjust table name as needed)\n",
    "    sales_df = spark.table(\"samples.bakehouse.sales_customers\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\n📋 Sample data:\")\n",
    "    sales_df.select(\"customer_id\", \"order_id\", \"product_name\", \"order_value\").show(5, truncate=False)\n",
    "    \n",
    "    # Process and send to Event Hub\n",
    "    process_dataframe_to_eventhub(\n",
    "        df=sales_df,\n",
    "        connection_string=EVENT_HUB_CONNECTION_STRING,\n",
    "        event_hub_name=EVENT_HUB_NAME,\n",
    "        batch_size=50,  # Smaller batches for demo\n",
    "        partition_strategy=\"customer_id\"  # Keep customer events together\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in Example 1: {e}\")\n",
    "    print(\"💡 Make sure the table 'samples.bakehouse.sales_customers' exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857adc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Alternative Examples for Different Data Sources ---\n",
    "\n",
    "# Example 2: Create sample data if the table doesn't exist\n",
    "print(\"\\n🔧 Example 2: Sample Data Generation\")\n",
    "\n",
    "# Create sample sales data for testing\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"order_value\", DoubleType(), True),\n",
    "    StructField(\"order_date\", TimestampType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Generate sample data\n",
    "sample_data = []\n",
    "regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "segments = [\"Premium\", \"Standard\", \"Basic\"]\n",
    "products = [\"Widget A\", \"Widget B\", \"Widget C\", \"Gadget X\", \"Gadget Y\"]\n",
    "\n",
    "for i in range(1000):  # Create 1000 sample records\n",
    "    sample_data.append((\n",
    "        i % 100 + 1,  # customer_id (1-100)\n",
    "        f\"ORD-{i:06d}\",  # order_id\n",
    "        random.choice(products),  # product_name\n",
    "        round(random.uniform(10.0, 500.0), 2),  # order_value\n",
    "        datetime.now() - timedelta(days=random.randint(0, 365)),  # order_date\n",
    "        random.choice(segments),  # customer_segment\n",
    "        random.choice(regions)  # region\n",
    "    ))\n",
    "\n",
    "# Create DataFrame\n",
    "sample_df = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "print(f\"✅ Created sample DataFrame with {sample_df.count():,} records\")\n",
    "sample_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Process the sample data with different partitioning strategies\n",
    "print(\"\\n🔧 Example 3: Testing Different Partitioning Strategies\")\n",
    "\n",
    "# Test with hash partitioning for even distribution\n",
    "print(\"\\n📊 Testing Hash Partitioning (even distribution):\")\n",
    "try:\n",
    "    process_dataframe_to_eventhub(\n",
    "        df=sample_df.limit(100),  # Process first 100 records for demo\n",
    "        connection_string=EVENT_HUB_CONNECTION_STRING,\n",
    "        event_hub_name=EVENT_HUB_NAME,\n",
    "        batch_size=20,\n",
    "        partition_strategy=\"hash\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Hash partitioning test failed: {e}\")\n",
    "\n",
    "# Test with customer_id partitioning to keep customer events together\n",
    "print(\"\\n👥 Testing Customer ID Partitioning (events grouped by customer):\")\n",
    "try:\n",
    "    process_dataframe_to_eventhub(\n",
    "        df=sample_df.filter(col(\"customer_id\") <= 10),  # First 10 customers only\n",
    "        connection_string=EVENT_HUB_CONNECTION_STRING,\n",
    "        event_hub_name=EVENT_HUB_NAME,\n",
    "        batch_size=15,\n",
    "        partition_strategy=\"customer_id\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Customer ID partitioning test failed: {e}\")\n",
    "\n",
    "print(\"\\n✅ Partitioning strategy tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbfb0e",
   "metadata": {},
   "source": [
    "# 🎯 Databricks Event Hub Integration - Key Features\n",
    "\n",
    "## ✅ **What This Script Provides:**\n",
    "\n",
    "### **1. Databricks-Optimized Design**\n",
    "- Uses `dbutils.secrets` for secure credential management\n",
    "- Leverages Spark DataFrames with `toLocalIterator()` for memory efficiency\n",
    "- Proper JSON serialization with `to_json()` and `struct()`\n",
    "- Progress tracking for large datasets\n",
    "\n",
    "### **2. Event Hub Best Practices**\n",
    "- **Chunking**: Handles large records by splitting into multiple events\n",
    "- **Compression**: Uses gzip to reduce data size\n",
    "- **Partitioning**: Multiple strategies for optimal throughput\n",
    "- **Retry Logic**: Exponential backoff for resilient sending\n",
    "- **Batching**: Configurable batch sizes for performance tuning\n",
    "\n",
    "### **3. Partitioning Strategies**\n",
    "- **Hash**: Even distribution across all partitions\n",
    "- **Customer ID**: Keep related events together (good for ordering)\n",
    "- **Round Robin**: Simple load balancing\n",
    "\n",
    "### **4. Error Handling & Monitoring**\n",
    "- Comprehensive logging and statistics\n",
    "- Graceful error handling with detailed error messages\n",
    "- Progress tracking for long-running operations\n",
    "\n",
    "## 🔧 **Configuration for Your Environment:**\n",
    "\n",
    "1. **Update Databricks Secrets:**\n",
    "   ```python\n",
    "   # Set these in your Databricks secret scope\n",
    "   dbutils.secrets.put(\"kv-scope\", \"eh-connstr\", \"YOUR_EVENT_HUB_CONNECTION_STRING\")\n",
    "   dbutils.secrets.put(\"kv-scope\", \"eh-name\", \"YOUR_EVENT_HUB_NAME\")\n",
    "   ```\n",
    "\n",
    "2. **Adjust Constants:**\n",
    "   ```python\n",
    "   PARTITION_COUNT = 32    # Match your Event Hub partition count\n",
    "   BATCH_SIZE = 100        # Optimize based on your data size\n",
    "   MAX_EVENT_BYTES = 900_000  # Adjust based on Event Hub tier\n",
    "   ```\n",
    "\n",
    "## 🚀 **Usage Patterns:**\n",
    "\n",
    "- **Small datasets**: Use higher batch sizes (500-1000)\n",
    "- **Large datasets**: Use moderate batch sizes (50-200) with progress tracking\n",
    "- **Real-time streaming**: Combine with Spark Structured Streaming\n",
    "- **Customer ordering**: Use \"customer_id\" partitioning strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
