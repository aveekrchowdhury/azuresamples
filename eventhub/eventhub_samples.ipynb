{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, gzip, time, uuid, random\n",
    "from typing import Iterable\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "from azure.eventhub.exceptions import EventHubError, OperationTimeoutError\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- Constants ---\n",
    "# Service limits: 1 MB/event (Standard/Dedicated), 256 KB (Basic). Leave overhead headroom.\n",
    "MAX_EVENT_BYTES = 900_000  # ~0.9 MB safety margin\n",
    "MAX_RETRIES = 7\n",
    "BASE_DELAY = 0.5  # seconds\n",
    "MAX_BACKOFF = 30  # seconds\n",
    "\n",
    "# --- Helpers ---\n",
    "def to_bytes(obj) -> bytes:\n",
    "    if isinstance(obj, (bytes, bytearray)):\n",
    "        return bytes(obj)\n",
    "    if isinstance(obj, str):\n",
    "        return obj.encode(\"utf-8\")\n",
    "    return json.dumps(obj, separators=(\",\", \":\")).encode(\"utf-8\")\n",
    "\n",
    "def gzip_bytes(b: bytes) -> bytes:\n",
    "    return gzip.compress(b)\n",
    "\n",
    "def chunk_bytes(b: bytes, max_size: int) -> Iterable[bytes]:\n",
    "    for i in range(0, len(b), max_size):\n",
    "        yield b[i : i + max_size]\n",
    "\n",
    "def send_with_retry(producer: EventHubProducerClient, batch):\n",
    "    delay = BASE_DELAY\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            producer.send_batch(batch)\n",
    "            return\n",
    "        except (EventHubError, OperationTimeoutError) as e:\n",
    "            # server busy / transient\n",
    "            jitter = random.uniform(0, delay * 0.25)\n",
    "            time.sleep(min(delay + jitter, MAX_BACKOFF))\n",
    "            delay = min(delay * 2, MAX_BACKOFF)\n",
    "            if attempt == MAX_RETRIES:\n",
    "                raise\n",
    "\n",
    "# --- Producer (use Key Vault in real code) ---\n",
    "EVENT_HUB_CONNECTION_STRING = dbutils.secrets.get(\"kv-scope\", \"eh-connstr\")\n",
    "EVENT_HUB_NAME = \"datatransferhub\"\n",
    "\n",
    "producer = EventHubProducerClient.from_connection_string(\n",
    "    conn_str=EVENT_HUB_CONNECTION_STRING,\n",
    "    eventhub_name=EVENT_HUB_NAME,\n",
    ")\n",
    "\n",
    "# Create a streaming iterator (avoids collect())\n",
    "rows = (row for row in spark.table(\"samples.bakehouse.sales_customers\")\n",
    "                 .select(col(\"value_hub\").cast(\"string\").alias(\"value\"))\n",
    "                 .toLocalIterator())\n",
    "\n",
    "with producer:\n",
    "    # Choose a partition key strategy; here we use a corr_id per row (adjust as needed)\n",
    "    # If you have a natural key (customerId, orderId), use that instead to keep ordering.\n",
    "    current_batch = None\n",
    "    current_pk = None\n",
    "\n",
    "    def flush():\n",
    "        nonlocal current_batch\n",
    "        if current_batch and current_batch.count > 0:\n",
    "            send_with_retry(producer, current_batch)\n",
    "        current_batch = None\n",
    "\n",
    "    for row in rows:\n",
    "        payload = row[\"value\"]\n",
    "        corr_id = str(uuid.uuid4())  # replace with stable key for your domain\n",
    "        pk = corr_id  # keep all chunks for a message in order on one partition\n",
    "\n",
    "        # Prepare bytes (compress first to minimize chunk count)\n",
    "        body_bytes = gzip_bytes(to_bytes(payload))\n",
    "\n",
    "        # If first event for this correlation id, (re)create batch with its partition key\n",
    "        if current_batch is None or pk != current_pk:\n",
    "            flush()\n",
    "            current_batch = producer.create_batch(partition_key=pk)\n",
    "            current_pk = pk\n",
    "\n",
    "        # Build chunked events with reassembly metadata\n",
    "        chunks = list(chunk_bytes(body_bytes, MAX_EVENT_BYTES))\n",
    "        total = len(chunks)\n",
    "        for idx, chunk in enumerate(chunks, start=1):\n",
    "            ev = EventData(chunk)\n",
    "            # App-level reassembly metadata (consumer must use this)\n",
    "            ev.content_type = \"application/json+gzip\"\n",
    "            ev.properties = {\n",
    "                \"corr_id\": corr_id,\n",
    "                \"chunk_index\": idx,\n",
    "                \"chunks_total\": total,\n",
    "                \"compressed\": True,\n",
    "                \"schema\": \"your-schema-v1\"\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                current_batch.add(ev)\n",
    "            except ValueError:\n",
    "                # Batch full: flush and start a new one for the same partition key\n",
    "                send_with_retry(producer, current_batch)\n",
    "                current_batch = producer.create_batch(partition_key=pk)\n",
    "                # If a single chunk can't fit (shouldn't happen with our MAX_EVENT_BYTES), re-raise\n",
    "                current_batch.add(ev)\n",
    "\n",
    "    # final flush\n",
    "    flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
